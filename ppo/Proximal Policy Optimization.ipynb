{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "Link to [arXiv paper](https://arxiv.org/pdf/1707.06347.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from itertools import chain\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Topology, Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "# Chooses the next move.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, n_out),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Value Network\n",
    "# Estimates the value of a state.\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_in, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "\n",
    "# These datasets sample *observations*, not rollouts.\n",
    "# Policy dataset returns (prev_state, action, reward, ret) tuples.\n",
    "class PolicyDataset(Dataset):\n",
    "    def __init__(self, experience):\n",
    "        super(PolicyDataset, self).__init__()\n",
    "        self._exp = []\n",
    "        for x in experience:\n",
    "            self._exp.extend(x)\n",
    "        self._length = len(self._exp)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._exp[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Returns, Advantages\n",
    "## Rollout and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward: 21.430, value loss: 240.249, policy loss: -9.423, ratio: 0.961, adv: 13.355\n",
      "avg reward: 18.870, value loss: 169.881, policy loss: -9.116, ratio: 0.876, adv: 10.703\n",
      "avg reward: 14.210, value loss: 72.340, policy loss: -5.641, ratio: 0.871, adv: 7.302\n",
      "avg reward: 13.470, value loss: 59.011, policy loss: -5.217, ratio: 0.869, adv: 6.503\n",
      "avg reward: 14.730, value loss: 68.046, policy loss: -7.001, ratio: 0.761, adv: 7.069\n",
      "avg reward: 13.310, value loss: 43.797, policy loss: -5.058, ratio: 0.747, adv: 6.157\n",
      "avg reward: 13.190, value loss: 34.896, policy loss: -3.851, ratio: 0.747, adv: 4.793\n",
      "avg reward: 12.640, value loss: 25.403, policy loss: -2.198, ratio: 0.606, adv: 2.566\n",
      "avg reward: 12.240, value loss: 21.614, policy loss: -0.765, ratio: 0.585, adv: 1.660\n",
      "avg reward: 11.730, value loss: 19.407, policy loss: -0.216, ratio: 0.702, adv: 0.861\n",
      "avg reward: 12.090, value loss: 19.544, policy loss: -0.269, ratio: 0.668, adv: 1.106\n",
      "avg reward: 11.830, value loss: 17.592, policy loss: -0.150, ratio: 0.698, adv: 1.536\n",
      "avg reward: 11.630, value loss: 16.250, policy loss: -0.205, ratio: 0.747, adv: 1.458\n",
      "avg reward: 11.610, value loss: 13.835, policy loss: -0.152, ratio: 0.746, adv: 0.903\n",
      "avg reward: 11.410, value loss: 11.806, policy loss: -0.086, ratio: 0.753, adv: 1.420\n",
      "avg reward: 11.480, value loss: 10.600, policy loss: -0.260, ratio: 0.721, adv: 1.453\n",
      "avg reward: 11.250, value loss: 7.981, policy loss: -0.080, ratio: 0.766, adv: 0.935\n",
      "avg reward: 11.780, value loss: 8.094, policy loss: -0.217, ratio: 0.801, adv: 0.760\n",
      "avg reward: 12.140, value loss: 7.277, policy loss: -0.215, ratio: 0.742, adv: 0.710\n",
      "avg reward: 13.100, value loss: 8.128, policy loss: -0.720, ratio: 0.632, adv: 1.270\n",
      "avg reward: 13.360, value loss: 7.965, policy loss: -0.030, ratio: 0.669, adv: 0.394\n",
      "avg reward: 14.220, value loss: 8.887, policy loss: -0.926, ratio: 0.682, adv: 1.384\n",
      "avg reward: 16.050, value loss: 11.307, policy loss: -1.465, ratio: 0.572, adv: 0.838\n",
      "avg reward: 19.030, value loss: 14.639, policy loss: -2.789, ratio: 0.397, adv: 0.540\n",
      "avg reward: 21.230, value loss: 14.506, policy loss: -3.602, ratio: 0.210, adv: 0.586\n",
      "avg reward: 24.570, value loss: 14.106, policy loss: -7.699, ratio: 0.316, adv: 1.298\n",
      "avg reward: 27.900, value loss: 9.380, policy loss: -10.373, ratio: 0.253, adv: 1.228\n",
      "avg reward: 32.440, value loss: 5.443, policy loss: -29.097, ratio: 0.136, adv: 1.843\n",
      "avg reward: 23.400, value loss: 2.407, policy loss: 23.935, ratio: 0.458, adv: -5.428\n",
      "avg reward: 37.950, value loss: 6.685, policy loss: nan, ratio: nan, adv: 6.846\n",
      "avg reward: 9.510, value loss: 53.805, policy loss: nan, ratio: nan, adv: -22.166\n",
      "avg reward: 9.140, value loss: 39.783, policy loss: nan, ratio: nan, adv: -0.076\n",
      "avg reward: 9.310, value loss: 28.021, policy loss: nan, ratio: nan, adv: -0.772\n",
      "avg reward: 9.420, value loss: 22.196, policy loss: nan, ratio: nan, adv: -2.014\n",
      "avg reward: 9.430, value loss: 16.043, policy loss: nan, ratio: nan, adv: -0.445\n",
      "avg reward: 9.460, value loss: 13.181, policy loss: nan, ratio: nan, adv: -1.306\n",
      "avg reward: 9.420, value loss: 9.913, policy loss: nan, ratio: nan, adv: -0.620\n",
      "avg reward: 9.350, value loss: 7.838, policy loss: nan, ratio: nan, adv: -1.136\n",
      "avg reward: 9.410, value loss: 6.919, policy loss: nan, ratio: nan, adv: -1.015\n",
      "avg reward: 9.310, value loss: 5.371, policy loss: nan, ratio: nan, adv: -0.967\n",
      "avg reward: 9.330, value loss: 4.260, policy loss: nan, ratio: nan, adv: -0.556\n",
      "avg reward: 9.400, value loss: 3.696, policy loss: nan, ratio: nan, adv: -0.750\n",
      "avg reward: 9.370, value loss: 3.222, policy loss: nan, ratio: nan, adv: -0.532\n",
      "avg reward: 9.410, value loss: 2.733, policy loss: nan, ratio: nan, adv: -0.503\n",
      "avg reward: 9.230, value loss: 2.060, policy loss: nan, ratio: nan, adv: -0.201\n",
      "avg reward: 9.280, value loss: 1.813, policy loss: nan, ratio: nan, adv: -0.369\n",
      "avg reward: 9.410, value loss: 1.729, policy loss: nan, ratio: nan, adv: -0.670\n",
      "avg reward: 9.340, value loss: 1.423, policy loss: nan, ratio: nan, adv: -0.200\n",
      "avg reward: 9.280, value loss: 1.391, policy loss: nan, ratio: nan, adv: -0.475\n",
      "avg reward: 9.240, value loss: 1.114, policy loss: nan, ratio: nan, adv: -0.360\n",
      "avg reward: 9.220, value loss: 1.003, policy loss: nan, ratio: nan, adv: 0.076\n",
      "avg reward: 9.370, value loss: 1.031, policy loss: nan, ratio: nan, adv: -0.329\n",
      "avg reward: 9.480, value loss: 1.003, policy loss: nan, ratio: nan, adv: -0.389\n",
      "avg reward: 9.290, value loss: 0.878, policy loss: nan, ratio: nan, adv: -0.030\n",
      "avg reward: 9.360, value loss: 0.747, policy loss: nan, ratio: nan, adv: -0.066\n",
      "avg reward: 9.250, value loss: 0.823, policy loss: nan, ratio: nan, adv: -0.252\n",
      "avg reward: 9.240, value loss: 0.689, policy loss: nan, ratio: nan, adv: 0.023\n",
      "avg reward: 9.340, value loss: 0.620, policy loss: nan, ratio: nan, adv: -0.151\n",
      "avg reward: 9.410, value loss: 0.740, policy loss: nan, ratio: nan, adv: -0.193\n",
      "avg reward: 9.340, value loss: 0.465, policy loss: nan, ratio: nan, adv: -0.007\n",
      "avg reward: 9.460, value loss: 0.650, policy loss: nan, ratio: nan, adv: -0.157\n",
      "avg reward: 9.390, value loss: 0.533, policy loss: nan, ratio: nan, adv: -0.082\n",
      "avg reward: 9.390, value loss: 0.611, policy loss: nan, ratio: nan, adv: 0.046\n",
      "avg reward: 9.300, value loss: 0.410, policy loss: nan, ratio: nan, adv: 0.034\n",
      "avg reward: 9.380, value loss: 0.551, policy loss: nan, ratio: nan, adv: -0.113\n",
      "avg reward: 9.400, value loss: 0.484, policy loss: nan, ratio: nan, adv: -0.068\n",
      "avg reward: 9.340, value loss: 0.569, policy loss: nan, ratio: nan, adv: 0.149\n",
      "avg reward: 9.260, value loss: 0.514, policy loss: nan, ratio: nan, adv: -0.098\n",
      "avg reward: 9.480, value loss: 0.604, policy loss: nan, ratio: nan, adv: -0.089\n",
      "avg reward: 9.430, value loss: 0.566, policy loss: nan, ratio: nan, adv: 0.181\n",
      "avg reward: 9.250, value loss: 0.433, policy loss: nan, ratio: nan, adv: -0.018\n",
      "avg reward: 9.300, value loss: 0.477, policy loss: nan, ratio: nan, adv: -0.083\n",
      "avg reward: 9.410, value loss: 0.535, policy loss: nan, ratio: nan, adv: -0.113\n",
      "avg reward: 9.310, value loss: 0.480, policy loss: nan, ratio: nan, adv: 0.027\n",
      "avg reward: 9.220, value loss: 0.460, policy loss: nan, ratio: nan, adv: -0.057\n",
      "avg reward: 9.300, value loss: 0.504, policy loss: nan, ratio: nan, adv: 0.064\n",
      "avg reward: 9.410, value loss: 0.576, policy loss: nan, ratio: nan, adv: 0.038\n",
      "avg reward: 9.430, value loss: 0.430, policy loss: nan, ratio: nan, adv: -0.082\n",
      "avg reward: 9.350, value loss: 0.398, policy loss: nan, ratio: nan, adv: -0.098\n",
      "avg reward: 9.270, value loss: 0.501, policy loss: nan, ratio: nan, adv: 0.150\n",
      "avg reward: 9.370, value loss: 0.492, policy loss: nan, ratio: nan, adv: -0.134\n",
      "avg reward: 9.210, value loss: 0.447, policy loss: nan, ratio: nan, adv: -0.054\n",
      "avg reward: 9.290, value loss: 0.457, policy loss: nan, ratio: nan, adv: 0.063\n",
      "avg reward: 9.390, value loss: 0.385, policy loss: nan, ratio: nan, adv: 0.007\n",
      "avg reward: 9.300, value loss: 0.521, policy loss: nan, ratio: nan, adv: 0.141\n",
      "avg reward: 9.420, value loss: 0.509, policy loss: nan, ratio: nan, adv: -0.120\n",
      "avg reward: 9.310, value loss: 0.509, policy loss: nan, ratio: nan, adv: 0.155\n",
      "avg reward: 9.390, value loss: 0.457, policy loss: nan, ratio: nan, adv: -0.181\n",
      "avg reward: 9.370, value loss: 0.591, policy loss: nan, ratio: nan, adv: 0.234\n",
      "avg reward: 9.380, value loss: 0.547, policy loss: nan, ratio: nan, adv: 0.166\n",
      "avg reward: 9.300, value loss: 0.455, policy loss: nan, ratio: nan, adv: -0.158\n",
      "avg reward: 9.430, value loss: 0.452, policy loss: nan, ratio: nan, adv: -0.047\n",
      "avg reward: 9.430, value loss: 0.377, policy loss: nan, ratio: nan, adv: -0.091\n",
      "avg reward: 9.330, value loss: 0.429, policy loss: nan, ratio: nan, adv: -0.011\n",
      "avg reward: 9.290, value loss: 0.489, policy loss: nan, ratio: nan, adv: 0.173\n",
      "avg reward: 9.350, value loss: 0.491, policy loss: nan, ratio: nan, adv: -0.204\n",
      "avg reward: 9.340, value loss: 0.466, policy loss: nan, ratio: nan, adv: -0.076\n",
      "avg reward: 9.400, value loss: 0.557, policy loss: nan, ratio: nan, adv: 0.034\n",
      "avg reward: 9.350, value loss: 0.632, policy loss: nan, ratio: nan, adv: 0.090\n",
      "avg reward: 9.240, value loss: 0.447, policy loss: nan, ratio: nan, adv: -0.061\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def _calculate_returns(rollouts, gamma, value, device):\n",
    "    \"\"\"\n",
    "    Modifies `rollouts` in-place from (state, action, reward) to (state, action, reward, return).\n",
    "    \"\"\"\n",
    "    for i, rollout in enumerate(rollouts):\n",
    "        current_return = 0\n",
    "        for j in reversed(range(len(rollout))):\n",
    "            state, action_dist, action, reward = rollout[j]\n",
    "            ret = reward + gamma*current_return\n",
    "            adv = ret - value(_prepare_numpy(state, device)).to('cpu')\n",
    "            rollouts[i][j] = (state, action_dist, action, reward, ret, adv)\n",
    "            current_return = ret\n",
    "\n",
    "def get_ratio(current_action_dist, old_action_dist, action):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        current_action_dist ((n,4) ndarray) - the batched action distributions.\n",
    "        old_action_dist ((n,4) ndarray): the batched action distributions, frozen.\n",
    "        action ((n,) ndarray): the actions taken at each observation.\n",
    "    Returns:\n",
    "        ratio ((n,) ndarray): (new likelihood)/(old likelihood)\n",
    "    \"\"\"\n",
    "    n = current_action_dist.shape[0]\n",
    "    current_likelihood = current_action_dist[range(n), action].unsqueeze(1)\n",
    "    old_likelihood = old_action_dist[range(n), action].unsqueeze(1)\n",
    "    ratio = current_likelihood/old_likelihood\n",
    "    return ratio\n",
    "\n",
    "def likelihood_fn(action_dist, action):\n",
    "    n = action_dist.shape[0]\n",
    "    likelihood = action_dist[range(n), action].unsqueeze(1)\n",
    "    return likelihood\n",
    "\n",
    "def _prepare_numpy(ndarray, device):\n",
    "    return torch.Tensor(ndarray).to(device)\n",
    "\n",
    "def _prepare_tensor_batch(tensor, device):\n",
    "    return tensor.detach().float().to(device)\n",
    "\n",
    "def main(policy, value):\n",
    "    # Initialize environment and networks\n",
    "    env = gym.make('CartPole-v0')\n",
    "    device = torch.device('cuda')\n",
    "    policy = policy.to(device)\n",
    "    value = value.to(device)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    epochs = 100\n",
    "    n_episodes = 100\n",
    "    max_episode_length = 200\n",
    "    gamma = 0.99\n",
    "    policy_epochs = 5\n",
    "    batch_size = 256\n",
    "    epsilon = 0.2\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    \n",
    "    # Optimizers\n",
    "    params = chain(value.parameters(), policy.parameters())\n",
    "    optimizer = optim.Adam(params, lr=lr) #, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training\n",
    "    for _ in range(epochs):\n",
    "        # Generate Rollouts\n",
    "        rollouts = []\n",
    "        rewards_per_rollout = []\n",
    "        for _ in range(n_episodes):\n",
    "            state = env.reset() # Reset environment each episode\n",
    "\n",
    "            rollout = []\n",
    "            reward_total = 0\n",
    "            for _ in range(max_episode_length):\n",
    "                # Compute policy probabilities\n",
    "                action_dist = policy(_prepare_numpy(state, device).unsqueeze(0))[0,:].to('cpu')\n",
    "                action_one_hot = np.random.multinomial(1, action_dist.cpu().detach().numpy())\n",
    "                action = np.argmax(action_one_hot)\n",
    "\n",
    "                # Take Action\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                state = state.astype(np.float32)\n",
    "                rollout.append((state, action_dist, action, reward))\n",
    "                reward_total += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            rollouts.append(rollout)\n",
    "            rewards_per_rollout.append(reward_total)\n",
    "        # End Rollouts\n",
    "        \n",
    "        # Prepare Data\n",
    "        _calculate_returns(rollouts, gamma, value, device) # modifies rollouts inplace\n",
    "        avg_reward = sum(rewards_per_rollout) / len(rewards_per_rollout)\n",
    "        experience_dataset = PolicyDataset(rollouts)\n",
    "        data_loader = DataLoader(experience_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "        # End Prepare Data\n",
    "        \n",
    "        # Train\n",
    "        value_criteria = nn.MSELoss()              \n",
    "        for _ in range(policy_epochs):\n",
    "            avg_value_loss = 0\n",
    "            avg_policy_loss = 0\n",
    "            for state, action_dist, action, reward, ret, adv in data_loader:\n",
    "                # state is an (n_timesteps,4) tensor; action/reward/ret are all (n_timestaps,) tensors\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Convert float64 to float32, detach tensors\n",
    "                state = _prepare_tensor_batch(state, device)\n",
    "                action_dist = _prepare_tensor_batch(action_dist, device)\n",
    "                action = _prepare_tensor_batch(action, device).long()\n",
    "                reward = _prepare_tensor_batch(reward, device)\n",
    "                ret = _prepare_tensor_batch(ret, device)\n",
    "                adv = _prepare_tensor_batch(adv, device)\n",
    "                \n",
    "                # Calculate the ratio term\n",
    "                current_action_dist = policy(state)\n",
    "                ratio = get_ratio(current_action_dist, action_dist, action)\n",
    "                \n",
    "                # Value Loss\n",
    "                expected_returns = value(state).view(-1)\n",
    "                value_loss = value_criteria(expected_returns, ret)\n",
    "        \n",
    "                # Proximal Policy Optimization\n",
    "#                 advantage = ret - expected_returns.detach()\n",
    "                advantage = adv\n",
    "                lhs = ratio * advantage\n",
    "                rhs = torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantage\n",
    "                policy_loss = -1*torch.mean(torch.min(lhs, rhs))\n",
    "        \n",
    "                # Policy gradient\n",
    "                likelihood = likelihood_fn(current_action_dist, action)\n",
    "                policy_loss = torch.mean(torch.log(likelihood)*advantage)\n",
    "                \n",
    "                # Logging\n",
    "                avg_value_loss += value_loss.item()\n",
    "                avg_policy_loss += policy_loss.item()\n",
    "                \n",
    "                # Backpropagate\n",
    "                loss = policy_loss + value_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "              \n",
    "            avg_value_loss /= len(data_loader)\n",
    "            avg_policy_loss /= len(data_loader)\n",
    "         \n",
    "        ratio_print = ratio.mean().item()\n",
    "        adv_print = advantage.mean().item()\n",
    "        ret_print = ret[0].item()\n",
    "        exp_ret_print = expected_returns.mean().item()\n",
    "#         print(\"ret: {:.3f}, exp_ret: {:.3f}\".format(ret_print, exp_ret_print))\n",
    "        print(\"avg reward: {:.3f}, value loss: {:.3f}, policy loss: {:.3f}, ratio: {:.3f}, adv: {:.3f}\".format(avg_reward, \n",
    "                                    avg_value_loss, avg_policy_loss, ratio_print, adv_print))\n",
    "\n",
    "n_in = 4\n",
    "n_out = 2  \n",
    "device = 'cuda'\n",
    "policy = PolicyNetwork(n_in, n_out).to(device)\n",
    "value = ValueNetwork(n_in).to(device)\n",
    "main(policy, value)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rollouts[0][:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
