{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/91/55/8cb23a97301b177e9c8e3226dba45bb454411de2cbd25746763267f226c2/tqdm-4.28.1-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.3MB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.28.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  23.75, value loss:  308.64, policy loss: -14.39:   0%|          | 1/1000 [00:01<18:07,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  23.57, value loss:  308.00, policy loss: -14.15:   0%|          | 2/1000 [00:02<17:42,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  25.64, value loss:  390.12, policy loss: -15.49:   0%|          | 3/1000 [00:03<17:34,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  26.98, value loss:  299.14, policy loss: -12.50:   0%|          | 4/1000 [00:04<17:38,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  34.87, value loss:  320.50, policy loss:  -6.95:   0%|          | 5/1000 [00:05<19:14,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  39.24, value loss:  332.84, policy loss:  -0.83:   1%|          | 6/1000 [00:07<21:11,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  64.91, value loss:  419.26, policy loss:  -0.54:   1%|          | 7/1000 [00:09<26:41,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  103.49, value loss:  393.65, policy loss:   0.27:   1%|          | 8/1000 [00:13<37:20,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  146.47, value loss:  321.88, policy loss:  -0.09:   1%|          | 9/1000 [00:18<52:36,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  177.74, value loss:  292.21, policy loss:   0.05:   1%|          | 10/1000 [00:25<1:08:51,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  184.26, value loss:  335.05, policy loss:  -0.09:   1%|          | 11/1000 [00:31<1:20:48,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  196.17, value loss:  315.48, policy loss:  -0.03:   1%|          | 12/1000 [00:38<1:31:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  188.19, value loss:  297.25, policy loss:  -0.05:   1%|▏         | 13/1000 [00:45<1:37:23,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.51, value loss:  271.16, policy loss:   0.01:   1%|▏         | 14/1000 [00:52<1:43:17,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  183.51, value loss:  282.42, policy loss:   0.03:   2%|▏         | 15/1000 [00:59<1:44:54,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.20, value loss:  253.54, policy loss:   0.17:   2%|▏         | 16/1000 [01:06<1:48:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.87, value loss:  280.57, policy loss:   0.02:   2%|▏         | 17/1000 [01:13<1:51:45,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.20, value loss:  258.06, policy loss:   0.07:   2%|▏         | 18/1000 [01:20<1:53:21,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  195.69, value loss:  246.79, policy loss:   0.02:   2%|▏         | 19/1000 [01:27<1:53:32,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  194.93, value loss:  202.84, policy loss:   0.10:   2%|▏         | 20/1000 [01:34<1:53:53,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  194.60, value loss:  235.41, policy loss:  -0.06:   2%|▏         | 21/1000 [01:41<1:53:48,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  200.00, value loss:  281.97, policy loss:  -0.07:   2%|▏         | 22/1000 [01:49<1:54:20,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.19, value loss:  279.58, policy loss:   0.10:   2%|▏         | 23/1000 [01:56<1:54:34,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.66, value loss:  221.64, policy loss:  -0.06:   2%|▏         | 24/1000 [02:03<1:55:06,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.63, value loss:  283.45, policy loss:   0.02:   2%|▎         | 25/1000 [02:10<1:56:08,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.87, value loss:  256.76, policy loss:  -0.08:   3%|▎         | 26/1000 [02:17<1:56:35,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  192.35, value loss:  154.14, policy loss:  -0.02:   3%|▎         | 27/1000 [02:24<1:55:39,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.02, value loss:  219.53, policy loss:   0.01:   3%|▎         | 28/1000 [02:32<1:55:42,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  195.97, value loss:  176.44, policy loss:   0.02:   3%|▎         | 29/1000 [02:39<1:55:22,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.34, value loss:  228.24, policy loss:  -0.04:   3%|▎         | 30/1000 [02:46<1:55:40,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.61, value loss:  197.96, policy loss:   0.05:   3%|▎         | 31/1000 [02:53<1:55:29,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  195.29, value loss:  290.91, policy loss:   0.06:   3%|▎         | 32/1000 [03:00<1:54:30,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  189.65, value loss:  293.21, policy loss:  -0.09:   3%|▎         | 33/1000 [03:07<1:53:58,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  196.38, value loss:  249.33, policy loss:   0.02:   3%|▎         | 34/1000 [03:14<1:54:20,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.26, value loss:  219.99, policy loss:   0.02:   4%|▎         | 35/1000 [03:22<1:55:55,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  198.55, value loss:  228.89, policy loss:  -0.01:   4%|▎         | 36/1000 [03:29<1:56:33,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.01, value loss:  241.78, policy loss:   0.01:   4%|▎         | 37/1000 [03:39<2:07:38,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.74, value loss:  261.69, policy loss:  -0.18:   4%|▍         | 38/1000 [03:48<2:14:58,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.81, value loss:  278.59, policy loss:   0.07:   4%|▍         | 39/1000 [03:58<2:21:13,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.49, value loss:  261.97, policy loss:  -0.00:   4%|▍         | 40/1000 [04:07<2:24:39,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  196.38, value loss:  201.16, policy loss:  -0.07:   4%|▍         | 41/1000 [04:17<2:27:01,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  199.20, value loss:  283.37, policy loss:  -0.03:   4%|▍         | 42/1000 [04:27<2:29:28,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  200.00, value loss:  268.01, policy loss:   0.14:   4%|▍         | 43/1000 [04:36<2:30:58,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg reward:  197.73, value loss:  291.46, policy loss:   0.09:   4%|▍         | 43/1000 [04:45<2:30:58,  9.47s/it]Process Process-261:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fe80edfedab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fe80edfedab3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleValueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     ppo(factory, policy, value, multinomial_likelihood, epochs=1000, rollouts_per_epoch=100, max_episode_length=200,\n\u001b[0;32m--> 329\u001b[0;31m         gamma=0.99, policy_epochs=5, batch_size=256)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fe80edfedab3>\u001b[0m in \u001b[0;36mppo\u001b[0;34m(env_factory, policy, value, likelihood_fn, embedding_net, epochs, rollouts_per_epoch, max_episode_length, gamma, policy_epochs, batch_size, epsilon, environment_threads, data_loader_threads, device, lr, betas, weight_decay, gif_name, gif_epochs, csv_file)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import imageio\n",
    "from itertools import chain\n",
    "import math\n",
    "from threading import Thread\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "class RLEnvironment(object):\n",
    "    \"\"\"An RL Environment, used for wrapping environments to run PPO on.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RLEnvironment, self).__init__()\n",
    "\n",
    "    def step(self, x):\n",
    "        \"\"\"Takes an action x, which is the same format as the output from a policy network.\n",
    "        Returns observation (np.ndarray), reward (float), terminal (boolean)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment.\n",
    "        Returns observation (np.ndarray)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class EnvironmentFactory(object):\n",
    "    \"\"\"Creates new environment objects\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EnvironmentFactory, self).__init__()\n",
    "\n",
    "    def new(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ExperienceDataset(Dataset):\n",
    "    def __init__(self, experience):\n",
    "        super(ExperienceDataset, self).__init__()\n",
    "        self._exp = []\n",
    "        for x in experience:\n",
    "            self._exp.extend(x)\n",
    "        self._length = len(self._exp)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._exp[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "\n",
    "def multinomial_likelihood(dist, idx):\n",
    "    return dist[range(dist.shape[0]), idx.long()[:, 0]].unsqueeze(1)\n",
    "\n",
    "\n",
    "def get_log_p(data, mu, sigma):\n",
    "    \"\"\"get negative log likelihood from normal distribution\"\"\"\n",
    "    return -torch.log(torch.sqrt(2 * math.pi * sigma ** 2)) - (data - mu) ** 2 / (2 * sigma ** 2)\n",
    "\n",
    "\n",
    "def ppo(env_factory, policy, value, likelihood_fn, embedding_net=None, epochs=1000, rollouts_per_epoch=100,\n",
    "        max_episode_length=200, gamma=0.99, policy_epochs=5, batch_size=256, epsilon=0.2, environment_threads=1,\n",
    "        data_loader_threads=1, device=torch.device('cpu'), lr=1e-3, betas=(0.9, 0.999), weight_decay=0.01, gif_name='',\n",
    "        gif_epochs=0, csv_file='latest_run.csv'):\n",
    "    # Clear the csv file\n",
    "    with open(csv_file, 'w') as f:\n",
    "        f.write('avg_reward, value_loss, policy_loss')\n",
    "\n",
    "    # Move networks to the correct device\n",
    "    policy = policy.to(device)\n",
    "    value = value.to(device)\n",
    "\n",
    "    # Collect parameters\n",
    "    params = chain(policy.parameters(), value.parameters())\n",
    "    if embedding_net:\n",
    "        embedding_net = embedding_net.to(device)\n",
    "        params = chain(params, embedding_net.parameters())\n",
    "\n",
    "    # Set up optimization\n",
    "    optimizer = optim.Adam(params, lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "    value_criteria = nn.MSELoss()\n",
    "\n",
    "    # Calculate the upper and lower bound for PPO\n",
    "    ppo_lower_bound = 1 - epsilon\n",
    "    ppo_upper_bound = 1 + epsilon\n",
    "\n",
    "    loop = tqdm(total=epochs, position=0, leave=False)\n",
    "\n",
    "    # Prepare the environments\n",
    "    environments = [env_factory.new() for _ in range(environment_threads)]\n",
    "    rollouts_per_thread = rollouts_per_epoch // environment_threads\n",
    "    remainder = rollouts_per_epoch % environment_threads\n",
    "    rollout_nums = ([rollouts_per_thread + 1] * remainder) + ([rollouts_per_thread] * (environment_threads - remainder))\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # Run the environments\n",
    "        experience_queue = Queue()\n",
    "        reward_queue = Queue()\n",
    "        threads = [Thread(target=_run_envs, args=(environments[i],\n",
    "                                                  embedding_net,\n",
    "                                                  policy,\n",
    "                                                  experience_queue,\n",
    "                                                  reward_queue,\n",
    "                                                  rollout_nums[i],\n",
    "                                                  max_episode_length,\n",
    "                                                  gamma,\n",
    "                                                  device)) for i in range(environment_threads)]\n",
    "        for x in threads:\n",
    "            x.start()\n",
    "        for x in threads:\n",
    "            x.join()\n",
    "\n",
    "        # Collect the experience\n",
    "        rollouts = list(experience_queue.queue)\n",
    "        avg_r = sum(reward_queue.queue) / reward_queue.qsize()\n",
    "        loop.set_description('avg reward: % 6.2f' % (avg_r))\n",
    "\n",
    "        # Make gifs\n",
    "        if gif_epochs and e % gif_epochs == 0:\n",
    "            _make_gif(rollouts[0], gif_name + '%d.gif' % e)\n",
    "\n",
    "        # Update the policy\n",
    "        experience_dataset = ExperienceDataset(rollouts)\n",
    "        data_loader = DataLoader(experience_dataset, num_workers=data_loader_threads, batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 pin_memory=False)\n",
    "        avg_policy_loss = 0\n",
    "        avg_val_loss = 0\n",
    "        for _ in range(policy_epochs):\n",
    "            avg_policy_loss = 0\n",
    "            avg_val_loss = 0\n",
    "            for state, old_action_dist, old_action, reward, ret in data_loader:\n",
    "                state = _prepare_tensor_batch(state, device)\n",
    "                old_action_dist = _prepare_tensor_batch(old_action_dist, device)\n",
    "                old_action = _prepare_tensor_batch(old_action, device)\n",
    "                ret = _prepare_tensor_batch(ret, device).unsqueeze(1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # If there is an embedding net, carry out the embedding\n",
    "                if embedding_net:\n",
    "                    state = embedding_net(state)\n",
    "\n",
    "                # Calculate the ratio term\n",
    "                current_action_dist = policy(state, False)\n",
    "                current_likelihood = likelihood_fn(current_action_dist, old_action)\n",
    "                old_likelihood = likelihood_fn(old_action_dist, old_action)\n",
    "                ratio = (current_likelihood / old_likelihood)\n",
    "\n",
    "                # Calculate the value loss\n",
    "                expected_returns = value(state)\n",
    "                val_loss = value_criteria(expected_returns, ret)\n",
    "\n",
    "                # Calculate the policy loss\n",
    "                advantage = ret - expected_returns.detach()\n",
    "                lhs = ratio * advantage\n",
    "                rhs = torch.clamp(ratio, ppo_lower_bound, ppo_upper_bound) * advantage\n",
    "                policy_loss = -torch.mean(torch.min(lhs, rhs))\n",
    "\n",
    "                # For logging\n",
    "                avg_val_loss += val_loss.item()\n",
    "                avg_policy_loss += policy_loss.item()\n",
    "\n",
    "                # Backpropagate\n",
    "                loss = policy_loss + val_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Log info\n",
    "            avg_val_loss /= len(data_loader)\n",
    "            avg_policy_loss /= len(data_loader)\n",
    "            loop.set_description(\n",
    "                'avg reward: % 6.2f, value loss: % 6.2f, policy loss: % 6.2f' % (avg_r, avg_val_loss, avg_policy_loss))\n",
    "        with open(csv_file, 'a+') as f:\n",
    "            f.write('%6.2f, %6.2f, %6.2f\\n' % (avg_r, avg_val_loss, avg_policy_loss))\n",
    "        print()\n",
    "        loop.update(1)\n",
    "\n",
    "\n",
    "def _calculate_returns(trajectory, gamma):\n",
    "    current_return = 0\n",
    "    for i in reversed(range(len(trajectory))):\n",
    "        state, action_dist, action, reward = trajectory[i]\n",
    "        ret = reward + gamma * current_return\n",
    "        trajectory[i] = (state, action_dist, action, reward, ret)\n",
    "        current_return = ret\n",
    "\n",
    "\n",
    "def _run_envs(env, embedding_net, policy, experience_queue, reward_queue, num_rollouts, max_episode_length,\n",
    "              gamma, device):\n",
    "    for _ in range(num_rollouts):\n",
    "        current_rollout = []\n",
    "        s = env.reset()\n",
    "        episode_reward = 0\n",
    "        for _ in range(max_episode_length):\n",
    "            input_state = _prepare_numpy(s, device)\n",
    "            if embedding_net:\n",
    "                input_state = embedding_net(input_state)\n",
    "\n",
    "            action_dist, action = policy(input_state)\n",
    "            action_dist, action = action_dist[0], action[0]  # Remove the batch dimension\n",
    "            s_prime, r, t = env.step(action)\n",
    "\n",
    "            if type(r) != float:\n",
    "                print('run envs:', r, type(r))\n",
    "\n",
    "            current_rollout.append((s, action_dist.cpu().detach().numpy(), action, r))\n",
    "            episode_reward += r\n",
    "            if t:\n",
    "                break\n",
    "            s = s_prime\n",
    "        _calculate_returns(current_rollout, gamma)\n",
    "        experience_queue.put(current_rollout)\n",
    "        reward_queue.put(episode_reward)\n",
    "\n",
    "\n",
    "def _prepare_numpy(ndarray, device):\n",
    "    return torch.from_numpy(ndarray).float().unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def _prepare_tensor_batch(tensor, device):\n",
    "    return tensor.detach().float().to(device)\n",
    "\n",
    "\n",
    "def _make_gif(rollout, filename):\n",
    "    with imageio.get_writer(filename, mode='I', duration=1 / 30) as writer:\n",
    "        for x in rollout:\n",
    "            writer.append_data((x[0][:, :, 0] * 255).astype(np.uint8))\n",
    "\n",
    "class CartPoleEnvironmentFactory(EnvironmentFactory):\n",
    "    def __init__(self):\n",
    "        super(CartPoleEnvironmentFactory, self).__init__()\n",
    "\n",
    "    def new(self):\n",
    "        return CartPoleEnvironment()\n",
    "\n",
    "\n",
    "class CartPoleEnvironment(RLEnvironment):\n",
    "    def __init__(self):\n",
    "        super(CartPoleEnvironment, self).__init__()\n",
    "        self._env = gym.make('CartPole-v0')\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"action is type np.ndarray of shape [1] and type np.uint8.\n",
    "        Returns observation (np.ndarray), r (float), t (boolean)\n",
    "        \"\"\"\n",
    "        s, r, t, _ = self._env.step(action.item())\n",
    "        return s, r, t\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns observation (np.ndarray)\"\"\"\n",
    "        return self._env.reset()\n",
    "\n",
    "\n",
    "class CartPolePolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy Network for CartPole.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim=4, action_dim=2):\n",
    "        super(CartPolePolicyNetwork, self).__init__()\n",
    "        self._net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, action_dim)\n",
    "        )\n",
    "        self._softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, get_action=True):\n",
    "        \"\"\"Receives input x of shape [batch, state_dim].\n",
    "        Outputs action distribution (categorical distribution) of shape [batch, action_dim],\n",
    "        as well as a sampled action (optional).\n",
    "        \"\"\"\n",
    "        scores = self._net(x)\n",
    "        probs = self._softmax(scores)\n",
    "\n",
    "        if not get_action:\n",
    "            return probs\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        actions = np.empty((batch_size, 1), dtype=np.uint8)\n",
    "        probs_np = probs.cpu().detach().numpy()\n",
    "        for i in range(batch_size):\n",
    "            action_one_hot = np.random.multinomial(1, probs_np[i])\n",
    "            action_idx = np.argmax(action_one_hot)\n",
    "            actions[i, 0] = action_idx\n",
    "        return probs, actions\n",
    "\n",
    "\n",
    "class CartPoleValueNetwork(nn.Module):\n",
    "    \"\"\"Approximates the value of a particular CartPole state.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim=4):\n",
    "        super(CartPoleValueNetwork, self).__init__()\n",
    "        self._net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Receives an observation of shape [batch, state_dim].\n",
    "        Returns the value of each state, in shape [batch, 1]\n",
    "        \"\"\"\n",
    "        return self._net(x)\n",
    "\n",
    "\n",
    "def main():\n",
    "    factory = CartPoleEnvironmentFactory()\n",
    "    policy = CartPolePolicyNetwork()\n",
    "    value = CartPoleValueNetwork()\n",
    "    ppo(factory, policy, value, multinomial_likelihood, epochs=1000, rollouts_per_epoch=100, max_episode_length=200,\n",
    "        gamma=0.99, policy_epochs=5, batch_size=256)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 24 13:13:48 2018       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN X (Pascal)    Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 23%   38C    P8    17W / 250W |  12146MiB / 12192MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| 23%   35C    P8    16W / 250W |   6614MiB / 12196MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1542      G   /usr/lib/xorg/Xorg                            24MiB |\r\n",
      "|    0      1726      G   /usr/bin/gnome-shell                          13MiB |\r\n",
      "|    0      2185      C   /usr/bin/python3                            7009MiB |\r\n",
      "|    0      3752      G   /usr/lib/xorg/Xorg                           176MiB |\r\n",
      "|    0      3890      G   /usr/bin/gnome-shell                         128MiB |\r\n",
      "|    0      4068      G   /usr/lib/xorg/Xorg                            77MiB |\r\n",
      "|    0      4201      G   /usr/bin/gnome-shell                         179MiB |\r\n",
      "|    0      4814      G   /usr/lib/firefox/firefox                       2MiB |\r\n",
      "|    0      5564      G   ...-token=16101378BB11523E56ABF2AC17A62A78   120MiB |\r\n",
      "|    0      8554      C   /usr/bin/python3                             969MiB |\r\n",
      "|    0     10817      C   /usr/bin/python3                             765MiB |\r\n",
      "|    0     27634      C   /usr/bin/python3                            1557MiB |\r\n",
      "|    0     27861      G   /usr/lib/xorg/Xorg                            24MiB |\r\n",
      "|    0     27948      G   /usr/bin/gnome-shell                         135MiB |\r\n",
      "|    0     29373      G   /usr/lib/xorg/Xorg                            49MiB |\r\n",
      "|    0     29533      G   /usr/bin/gnome-shell                         145MiB |\r\n",
      "|    0     30094      G   ...-token=930129C71BA85B66E81C92C088DC8CF6   143MiB |\r\n",
      "|    1      2185      C   /usr/bin/python3                            6547MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
