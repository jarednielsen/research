{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 2 - Proximal Policy Optimization\n",
    "\n",
    "**GOAL:** The goal of this exercise is to demonstrate how to use the proximal policy optimization (PPO) algorithm.\n",
    "\n",
    "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
    "\n",
    "PPO is described in detail in https://arxiv.org/abs/1707.06347. It is a variant of Trust Region Policy Optimization (TRPO) described in https://arxiv.org/abs/1502.05477\n",
    "\n",
    "PPO works in two phases. In one phase, a large number of rollouts are performed (in parallel). The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. We then use SGD to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![ppo](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/ppo.png)\n",
    "\n",
    "**NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary (for this to work, you must be using a machine that has GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOAgent, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray. This must be done before we instantiate any RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-01-24_17-57-27_29893/logs.\n",
      "Waiting for redis server at 127.0.0.1:45208 to respond...\n",
      "Waiting for redis server at 127.0.0.1:12644 to respond...\n",
      "Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8890/notebooks/ray_ui.ipynb?token=489d93316da6e238de39d7b709a8dbc50ea6f352da6a5be8\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.23.45',\n",
       " 'redis_address': '192.168.23.45:45208',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-01-24_17-57-27_29893/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-01-24_17-57-27_29893/sockets/raylet'],\n",
       " 'webui_url': 'http://localhost:8890/notebooks/ray_ui.ipynb?token=489d93316da6e238de39d7b709a8dbc50ea6f352da6a5be8'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a PPOAgent object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
    "- `num_sgd_iter` is the number of epochs of SGD (passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO.\n",
    "- `sgd_minibatch_size` is the SGD batch size that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jared/ray_results/PPO_CartPole-v0_2019-01-24_17-57-28dot8dw05 -> None\n",
      "2019-01-24 17:57:28,055\tWARNING ppo.py:118 -- By default, observations will be normalized with MeanStdFilter\n",
      "/home/jared/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/home/jared/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2019-01-24 17:57:29,251\tINFO multi_gpu_optimizer.py:62 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "agent = PPOAgent(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the policy on the `CartPole-v0` environment for 2 steps. The CartPole problem is described at https://gym.openai.com/envs/CartPole-v0.\n",
    "\n",
    "**EXERCISE:** Inspect how well the policy is doing by looking for the lines that say something like\n",
    "\n",
    "```\n",
    "total reward is  22.3215974777\n",
    "trajectory length mean is  21.3215974777\n",
    "```\n",
    "\n",
    "This indicates how much reward the policy is receiving and how many time steps of the environment the policy ran. The maximum possible reward for this problem is 200. The reward and trajectory length are very close because the agent receives a reward of one for every time step that it survives (however, that is specific to this environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/.conda/envs/ray-tutorial/lib/python3.6/site-packages/ray/tune/logger.py:188: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(value, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2019-01-24_17-57-40\n",
      "done: false\n",
      "episode_len_mean: 21.409523809523808\n",
      "episode_reward_max: 60.0\n",
      "episode_reward_mean: 21.409523809523808\n",
      "episode_reward_min: 8.0\n",
      "episodes_this_iter: 210\n",
      "episodes_total: 210\n",
      "experiment_id: e1c8873880fa47cca619a31a8b3399f6\n",
      "hostname: santaka\n",
      "info:\n",
      "  cur_lr: 4.999999873689376e-05\n",
      "  entropy: 0.662660539150238\n",
      "  grad_time_ms: 2363.136\n",
      "  kl: 0.032554734498262405\n",
      "  load_time_ms: 67.202\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "  policy_loss: -0.041429586708545685\n",
      "  sample_time_ms: 1610.597\n",
      "  total_loss: 161.13922119140625\n",
      "  update_time_ms: 784.02\n",
      "  vf_explained_var: 0.03205602988600731\n",
      "  vf_loss: 161.1741485595703\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.23.45\n",
      "num_metric_batches_dropped: 0\n",
      "pid: 29893\n",
      "policy_reward_mean: {}\n",
      "time_since_restore: 4.8688483238220215\n",
      "time_this_iter_s: 4.8688483238220215\n",
      "time_total_s: 4.8688483238220215\n",
      "timestamp: 1548377860\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-01-24_17-57-43\n",
      "done: false\n",
      "episode_len_mean: 40.16379310344828\n",
      "episode_reward_max: 111.0\n",
      "episode_reward_mean: 40.16379310344828\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 116\n",
      "episodes_total: 326\n",
      "experiment_id: e1c8873880fa47cca619a31a8b3399f6\n",
      "hostname: santaka\n",
      "info:\n",
      "  cur_lr: 4.999999873689376e-05\n",
      "  entropy: 0.6144771575927734\n",
      "  grad_time_ms: 2162.969\n",
      "  kl: 0.020250629633665085\n",
      "  load_time_ms: 34.042\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "  policy_loss: -0.033976756036281586\n",
      "  sample_time_ms: 1482.852\n",
      "  total_loss: 241.87106323242188\n",
      "  update_time_ms: 393.755\n",
      "  vf_explained_var: 0.08649657666683197\n",
      "  vf_loss: 241.8989715576172\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.23.45\n",
      "num_metric_batches_dropped: 0\n",
      "pid: 29893\n",
      "policy_reward_mean: {}\n",
      "time_since_restore: 8.200455904006958\n",
      "time_this_iter_s: 3.3316075801849365\n",
      "time_total_s: 8.200455904006958\n",
      "timestamp: 1548377863\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** The current network and training configuration are too large and heavy-duty for a simple problem like CartPole. Modify the configuration to use a smaller network and to speed up the optimization of the surrogate objective (fewer SGD iterations and a larger batch size should help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jared/ray_results/PPO_CartPole-v0_2019-01-24_17-57-43z6o57v08 -> None\n",
      "2019-01-24 17:57:43,608\tWARNING ppo.py:118 -- By default, observations will be normalized with MeanStdFilter\n",
      "2019-01-24 17:57:44,853\tINFO multi_gpu_optimizer.py:62 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    }
   ],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOAgent(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
    "\n",
    "This should take around 20 or 30 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2019-01-24_17-57-51\n",
      "done: false\n",
      "episode_len_mean: 21.956521739130434\n",
      "episode_reward_max: 56.0\n",
      "episode_reward_mean: 21.956521739130434\n",
      "episode_reward_min: 9.0\n",
      "episodes_this_iter: 207\n",
      "episodes_total: 207\n",
      "experiment_id: f8999da60abf40ca97844bc098ba9571\n",
      "hostname: santaka\n",
      "info:\n",
      "  cur_lr: 4.999999873689376e-05\n",
      "  entropy: 0.6624599099159241\n",
      "  grad_time_ms: 2663.643\n",
      "  kl: 0.0321684293448925\n",
      "  load_time_ms: 36.153\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "  policy_loss: -0.04183917120099068\n",
      "  sample_time_ms: 985.151\n",
      "  total_loss: 162.8946533203125\n",
      "  update_time_ms: 364.865\n",
      "  vf_explained_var: 0.050697725266218185\n",
      "  vf_loss: 162.9300994873047\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.23.45\n",
      "num_metric_batches_dropped: 0\n",
      "pid: 29893\n",
      "policy_reward_mean: {}\n",
      "time_since_restore: 4.083256721496582\n",
      "time_this_iter_s: 4.083256721496582\n",
      "time_total_s: 4.083256721496582\n",
      "timestamp: 1548377871\n",
      "timesteps_since_restore: 4000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2019-01-24_17-57-54\n",
      "done: false\n",
      "episode_len_mean: 43.30769230769231\n",
      "episode_reward_max: 162.0\n",
      "episode_reward_mean: 43.30769230769231\n",
      "episode_reward_min: 11.0\n",
      "episodes_this_iter: 104\n",
      "episodes_total: 311\n",
      "experiment_id: f8999da60abf40ca97844bc098ba9571\n",
      "hostname: santaka\n",
      "info:\n",
      "  cur_lr: 4.999999873689376e-05\n",
      "  entropy: 0.6129658818244934\n",
      "  grad_time_ms: 2521.681\n",
      "  kl: 0.018886111676692963\n",
      "  load_time_ms: 18.638\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "  policy_loss: -0.030802255496382713\n",
      "  sample_time_ms: 986.735\n",
      "  total_loss: 407.77447509765625\n",
      "  update_time_ms: 184.05\n",
      "  vf_explained_var: 0.05248421058058739\n",
      "  vf_loss: 407.79962158203125\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.23.45\n",
      "num_metric_batches_dropped: 0\n",
      "pid: 29893\n",
      "policy_reward_mean: {}\n",
      "time_since_restore: 7.4656336307525635\n",
      "time_this_iter_s: 3.3823769092559814\n",
      "time_total_s: 7.4656336307525635\n",
      "timestamp: 1548377874\n",
      "timesteps_since_restore: 8000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the current model. The call to `agent.save()` returns the path to the checkpointed model and can be used later to restore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jared/ray_results/PPO_CartPole-v0_2019-01-24_17-57-43z6o57v08/checkpoint_2/checkpoint-2\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the trained policy to make predictions.\n",
    "\n",
    "**NOTE:** Here we are loading the trained policy in the same process, but in practice, this would often be done in a different process (probably on a different machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jared/ray_results/PPO_CartPole-v0_2019-01-24_17-57-54gp29qa4v -> None\n",
      "2019-01-24 17:57:54,595\tWARNING ppo.py:118 -- By default, observations will be normalized with MeanStdFilter\n",
      "2019-01-24 17:57:55,394\tINFO multi_gpu_optimizer.py:62 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOAgent(trained_config, 'CartPole-v0')\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action.\n",
    "\n",
    "**EXERCISE:** Verify that the reward received roughly matches up with the reward printed in the training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results with TensorBoard\n",
    "\n",
    "**EXERCISE**: Finally, you can visualize your training results using TensorBoard. To do this, open a new terminal in Jupyter lab using the \"+\" button, and run:\n",
    "    \n",
    "`$ tensorboard --logdir=~/ray_results --host=0.0.0.0`\n",
    "\n",
    "And open your browser to the address printed (or change the current URL to go to port 6006). Check the \"episode_reward_mean\" learning curve of the PPO agent. Toggle the horizontal axis between both the \"STEPS\" and \"RELATIVE\" view to compare efficiency in number of timesteps vs real time time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
