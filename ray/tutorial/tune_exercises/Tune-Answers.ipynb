{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tune.png\" alt=\"Tune Logo\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is a scalable framework for model training and hyperparameter search with a focus on deep learning and deep reinforcement learning.\n",
    "\n",
    "**Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune\n",
    "\n",
    "**Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "\n",
    "**Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "\n",
    "**Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Tuning hyperparameters is often the most expensive part of the machine learning workflow. Tune is built to address this, demonstrating an efficient and scalable solution for this pain point.\n",
    "\n",
    "\n",
    "## Outline\n",
    "This tutorial will walk you through the following process:\n",
    "\n",
    "1. Creating and training a model on a toy dataset (MNIST)\n",
    "2. Integrating Tune into your workflow\n",
    "3. Trying out advanced features - plugging in an efficient scheduler\n",
    "4. Validating your trained model\n",
    "5. (Optional) Try out a search algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "limit_threads(4)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Creating a model to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Convolutional Neural Network model. Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. The details of how a Convolutional Neural Network works are unimportant here, but you're welcome to read more about them here: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "<img src=\"cnn.png\" alt=\"MNIST Visualization\" width=\"800\"/>\n",
    "\n",
    "\n",
    "This convolutional neural network model will be used classify digits (from the MNIST dataset).\n",
    "\n",
    "<img src=\"mnist.png\" alt=\"MNIST Visualization\" width=\"400\"/>\n",
    "\n",
    "This is a fairly simple dataset, but it enables us to explore Tune's functionality in depth.\n",
    "We will use 60,000 images to train the network. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the digit the image represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll specify some arguments and some reasonable defaults for this model. These are the hyperparameters settings that we will later use to further optimize this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Keras MNIST Example')\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, help='SGD momentum')\n",
    "parser.add_argument('--kernel1', type=int, default=3, help='Size of first kernel')\n",
    "parser.add_argument('--kernel2', type=int, default=3, help='Size of second kernel')\n",
    "parser.add_argument('--poolsize', type=int, default=2, help='Size of Poolin')\n",
    "parser.add_argument('--dropout1', type=float, default=0.25, help='Size of first kernel')\n",
    "parser.add_argument('--hidden', type=int, default=4, help='Size of Hidden Layer')\n",
    "parser.add_argument('--dropout2', type=float, default=0.5, help='Size of first kernel')\n",
    "\n",
    "DEFAULT_ARGS = vars(parser.parse_known_args()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below function will create and return a Convolutional Neural Network. You don't need to modify this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(parameters):\n",
    "    config = DEFAULT_ARGS.copy()  # This is obtained via the global scope\n",
    "    config.update(parameters)\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(config[\"kernel1\"], config[\"kernel1\"]),\n",
    "                     activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (config[\"kernel2\"], config[\"kernel2\"]), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(config[\"poolsize\"], config[\"poolsize\"])))\n",
    "    model.add(Dropout(config[\"dropout1\"]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config[\"hidden\"], activation='relu'))\n",
    "    model.add(Dropout(config[\"dropout2\"]))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(\n",
    "                      lr=config[\"lr\"], momentum=config[\"momentum\"]),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Setup a basic model training script.\n",
    "\n",
    "The process of training the neural network model occurs as follows:\n",
    "\n",
    "1. Feed the training data to the modelâ€”in this example, the `batch_of_data` and `batch_of_labels` arrays.\n",
    "2. The model learns to associate images and labels.\n",
    "\n",
    "**Exercise**: Finish the TODOs below. Here are a few hints:\n",
    "\n",
    "1) `data_generator` is an iterator that returns (`batch_of_data`, `batch_of_labels`), like follows:\n",
    "\n",
    "```python\n",
    "for batch_of_data, batch_of_labels in data_generator:\n",
    "    do_something_interesting()\n",
    "```\n",
    "2) You can use `model.fit(batch_of_data, batch_of_labels)` to repeatedly improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(args):\n",
    "    \"\"\"Loads data, does one pass over the data, and saves the weights.\"\"\"\n",
    "    data_generator = load_data()\n",
    "    model = make_model(args)\n",
    "    for batch_of_data, batch_of_labels in data_generator:\n",
    "        model.fit(batch_of_data, batch_of_labels)\n",
    "    ## TODO: use the `data_generator` to iterate over the data and improve the model\n",
    "    model.save_weights(\"./weights.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this above training script to make sure things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 2.2902 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 717us/step - loss: 2.3203 - acc: 0.0625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 2.3034 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 713us/step - loss: 2.3019 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 2.2836 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 2.2628 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 674us/step - loss: 2.4432 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 2.2875 - acc: 0.0938\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 2.2945 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 756us/step - loss: 2.3031 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 821us/step - loss: 2.2803 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 772us/step - loss: 2.2677 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 817us/step - loss: 2.3028 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 821us/step - loss: 2.2853 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 751us/step - loss: 2.3076 - acc: 0.0938\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 2.2799 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 2.2879 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 2.2695 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 678us/step - loss: 2.2236 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 810us/step - loss: 2.2534 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 828us/step - loss: 2.1924 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 783us/step - loss: 2.2452 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 830us/step - loss: 2.2208 - acc: 0.0625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 829us/step - loss: 2.2393 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 843us/step - loss: 2.2391 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 843us/step - loss: 2.1166 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 860us/step - loss: 2.2776 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 850us/step - loss: 2.2214 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 857us/step - loss: 2.1435 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 725us/step - loss: 2.0667 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 2.3827 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 2.2040 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 2.1418 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 2.0440 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 2.2901 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 2.2078 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 2.1956 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 713us/step - loss: 2.0654 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 2.2686 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 715us/step - loss: 2.1533 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 2.1047 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 726us/step - loss: 2.1598 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 673us/step - loss: 2.0443 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 2.2609 - acc: 0.0938\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 2.1326 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 2.1108 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 711us/step - loss: 2.0631 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 2.0908 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 2.1027 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 2.1850 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.9469 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 2.0360 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.9971 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 725us/step - loss: 2.0041 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 2.0929 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.9516 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.8232 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9644 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 2.2880 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 2.0190 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 2.0234 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 1.9752 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 2.0506 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.9484 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 2.0120 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 2.1989 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 2.0228 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 710us/step - loss: 1.9373 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 2.0672 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 715us/step - loss: 1.9224 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 2.0259 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 709us/step - loss: 1.8955 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 720us/step - loss: 2.1037 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.9889 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 719us/step - loss: 1.9948 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.9502 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 2.0210 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 677us/step - loss: 2.1575 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.9365 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 2.0832 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.8310 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 841us/step - loss: 2.1756 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 833us/step - loss: 1.9932 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 771us/step - loss: 2.0712 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 783us/step - loss: 1.9274 - acc: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 2.0431 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 1.7332 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 709us/step - loss: 2.0086 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 713us/step - loss: 2.0966 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 1.7702 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.9782 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 710us/step - loss: 1.9527 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 837us/step - loss: 1.8396 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 878us/step - loss: 2.0061 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 710us/step - loss: 1.9721 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 711us/step - loss: 2.5120 - acc: 0.0625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.7149 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 2.0263 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 763us/step - loss: 1.9479 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 709us/step - loss: 2.0976 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.7830 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 1.6816 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.9179 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 2.0309 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 763us/step - loss: 1.9952 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 760us/step - loss: 1.7515 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 717us/step - loss: 1.8735 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 722us/step - loss: 1.9952 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 708us/step - loss: 1.7269 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 2.0666 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 2.0589 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.9831 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 2.0736 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.8794 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 2.0276 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 724us/step - loss: 1.8354 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 678us/step - loss: 1.5851 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 679us/step - loss: 1.8681 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.9354 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.8545 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.9780 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.8965 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9372 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.7643 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 2.0610 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.8774 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 1.9178 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 677us/step - loss: 1.8046 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.7905 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 1.8337 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 2.0592 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.9435 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 1.7405 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 716us/step - loss: 1.7821 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 2.1340 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 2.0013 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 2.1081 - acc: 0.1250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.7872 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.9846 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 673us/step - loss: 2.0661 - acc: 0.0938\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 674us/step - loss: 1.8085 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.8802 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.9202 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.7842 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.6810 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.7528 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.9158 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.7943 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.7880 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8680 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 761us/step - loss: 1.7436 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 803us/step - loss: 1.7746 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 802us/step - loss: 1.9671 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 711us/step - loss: 1.7726 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.8367 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 2.1064 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 669us/step - loss: 1.9129 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.8427 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 676us/step - loss: 1.7028 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 709us/step - loss: 1.6926 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 2.0777 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.8346 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.7008 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 679us/step - loss: 2.0004 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.9204 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 676us/step - loss: 1.6174 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 669us/step - loss: 2.0753 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.8216 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.6596 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.7049 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.6946 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 1.5609 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 2.0556 - acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.9861 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 679us/step - loss: 1.6968 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 2.2386 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.6280 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.6917 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 678us/step - loss: 1.8225 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.7182 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.6496 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.6124 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.6816 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9320 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 2.0732 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 2.1075 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.8046 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.8277 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.8093 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.7491 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.6218 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 2.1114 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.6911 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.8094 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8725 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 674us/step - loss: 1.8420 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 755us/step - loss: 1.9275 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8148 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.6662 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.8074 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 676us/step - loss: 1.6901 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 663us/step - loss: 1.7473 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.6993 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 2.0398 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.8483 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.8304 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.9548 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.6361 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.7429 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.9032 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 670us/step - loss: 1.8199 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 2.0410 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.8043 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.9457 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.8765 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.6612 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.6276 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.8997 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 715us/step - loss: 1.7731 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 2.0919 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 802us/step - loss: 1.8013 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 733us/step - loss: 1.7178 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 732us/step - loss: 1.7397 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 738us/step - loss: 1.8447 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.7761 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 1.5472 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 1.5210 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 739us/step - loss: 1.7811 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.9811 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.8684 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 730us/step - loss: 1.5104 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.9030 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 1.5339 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 1.6731 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 1.6132 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 1.8496 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 748us/step - loss: 1.6567 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.6112 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 721us/step - loss: 1.6356 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 769us/step - loss: 1.9264 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.7928 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.8881 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 713us/step - loss: 1.5003 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.7927 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 706us/step - loss: 2.0937 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 715us/step - loss: 1.7857 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.4769 - acc: 0.5938\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.9514 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 722us/step - loss: 1.9425 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 719us/step - loss: 1.8271 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 717us/step - loss: 1.8395 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.7081 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 1.9282 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.5985 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 1.8602 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 733us/step - loss: 1.8955 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 739us/step - loss: 1.9424 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 2.0539 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 1.8791 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 738us/step - loss: 1.8104 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 773us/step - loss: 1.8612 - acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.5942 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.6086 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 756us/step - loss: 1.7509 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 726us/step - loss: 1.7363 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 761us/step - loss: 1.7374 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 724us/step - loss: 1.9309 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 739us/step - loss: 1.5902 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 763us/step - loss: 1.9598 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 767us/step - loss: 1.8094 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 760us/step - loss: 1.9363 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 735us/step - loss: 1.6872 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.8898 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 759us/step - loss: 1.8721 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 764us/step - loss: 1.9123 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 722us/step - loss: 1.6342 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 1.7090 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 719us/step - loss: 1.6932 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 731us/step - loss: 1.8755 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 730us/step - loss: 1.8408 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 756us/step - loss: 1.5979 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.7421 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 751us/step - loss: 1.8659 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 757us/step - loss: 2.0666 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 761us/step - loss: 1.7417 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 732us/step - loss: 1.6723 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 755us/step - loss: 1.7365 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 735us/step - loss: 1.9165 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 725us/step - loss: 1.7905 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 759us/step - loss: 2.1806 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.9537 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 747us/step - loss: 1.7012 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 755us/step - loss: 1.8618 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 746us/step - loss: 1.8797 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 749us/step - loss: 1.6404 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 747us/step - loss: 1.5356 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 725us/step - loss: 2.0507 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 757us/step - loss: 1.6545 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.8619 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 717us/step - loss: 1.6108 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 735us/step - loss: 1.6976 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 1.5410 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 762us/step - loss: 1.7424 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 724us/step - loss: 1.7433 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 751us/step - loss: 1.7133 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.8934 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 720us/step - loss: 1.7207 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 739us/step - loss: 1.7743 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.8701 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 751us/step - loss: 2.0282 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 754us/step - loss: 1.8659 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 802us/step - loss: 2.4128 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.6359 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 722us/step - loss: 1.5804 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 738us/step - loss: 1.7120 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 735us/step - loss: 2.0641 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 1.7390 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 2.0383 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.6629 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 748us/step - loss: 1.5684 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 791us/step - loss: 1.7602 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 720us/step - loss: 1.8337 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 759us/step - loss: 1.9278 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.6912 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.7361 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 1.8296 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 719us/step - loss: 1.7990 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 720us/step - loss: 1.7289 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 726us/step - loss: 1.7177 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.6722 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.7459 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 739us/step - loss: 1.7250 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.7599 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 2.0923 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.6190 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 1.8253 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 710us/step - loss: 1.7542 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.6662 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.6961 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.4488 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 864us/step - loss: 1.8294 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 879us/step - loss: 1.8117 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 887us/step - loss: 1.5611 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 910us/step - loss: 1.9172 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 798us/step - loss: 1.9270 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.5124 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.5169 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.8086 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.8882 - acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 723us/step - loss: 1.8867 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.8268 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 764us/step - loss: 1.6285 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 778us/step - loss: 1.9093 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 747us/step - loss: 1.6210 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 745us/step - loss: 1.6783 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 746us/step - loss: 1.9119 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 1.8437 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 732us/step - loss: 1.7931 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 730us/step - loss: 1.9627 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 1.6171 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 744us/step - loss: 1.4466 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.6491 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.6507 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 738us/step - loss: 1.7057 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 771us/step - loss: 1.7885 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 769us/step - loss: 1.4282 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 748us/step - loss: 1.5856 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.8450 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.6658 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 761us/step - loss: 1.5759 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 783us/step - loss: 1.8181 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 772us/step - loss: 1.4974 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 817us/step - loss: 1.8132 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.3861 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.5727 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.5828 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 712us/step - loss: 1.5580 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 738us/step - loss: 1.8742 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 754us/step - loss: 1.9086 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 714us/step - loss: 1.7989 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.6981 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 746us/step - loss: 1.7984 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 767us/step - loss: 1.8170 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 757us/step - loss: 1.9287 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 1.6152 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 1.7303 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 744us/step - loss: 1.8181 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 752us/step - loss: 1.7609 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 749us/step - loss: 2.0666 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 750us/step - loss: 1.7387 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.7175 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 751us/step - loss: 1.6951 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.4590 - acc: 0.5625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 753us/step - loss: 1.7782 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 762us/step - loss: 1.6032 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 754us/step - loss: 1.6923 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 740us/step - loss: 1.7202 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.8121 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 741us/step - loss: 1.6332 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 726us/step - loss: 1.7859 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 719us/step - loss: 1.7009 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 1.8043 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 734us/step - loss: 1.8510 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.6800 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 742us/step - loss: 1.8524 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 768us/step - loss: 1.7990 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 728us/step - loss: 1.7270 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 1.8455 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 724us/step - loss: 1.5445 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 736us/step - loss: 1.7399 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 743us/step - loss: 1.7146 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 725us/step - loss: 1.8243 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 722us/step - loss: 1.8733 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.9785 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 723us/step - loss: 1.7193 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.8936 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.4514 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.6142 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.6135 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.7761 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 708us/step - loss: 1.8342 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 1.7787 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.7260 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 671us/step - loss: 1.8059 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.5566 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.5410 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.7138 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 760us/step - loss: 1.7450 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 837us/step - loss: 1.7907 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 825us/step - loss: 1.6726 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 679us/step - loss: 1.7596 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 672us/step - loss: 1.9021 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 715us/step - loss: 1.8624 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 831us/step - loss: 1.4472 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 831us/step - loss: 2.0230 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.6917 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 672us/step - loss: 2.0136 - acc: 0.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.5438 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.9281 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 708us/step - loss: 1.5382 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 1.3608 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.9194 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 666us/step - loss: 1.7047 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 671us/step - loss: 2.0698 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.7298 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.5405 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.4769 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 667us/step - loss: 1.8165 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.8753 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 671us/step - loss: 1.7138 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.7848 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 664us/step - loss: 1.7356 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.8070 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.6495 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 668us/step - loss: 1.7241 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.7556 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 674us/step - loss: 1.8959 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.6000 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 663us/step - loss: 1.3954 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.8970 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 668us/step - loss: 1.5130 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 1.5520 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.7094 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.6505 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.6587 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 663us/step - loss: 1.6754 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 675us/step - loss: 1.8997 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.5902 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.8971 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.6383 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.6087 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.6527 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.5762 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 723us/step - loss: 1.8021 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.6196 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.6128 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 1.6329 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.9078 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 1.5321 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.8212 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.7925 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 707us/step - loss: 2.0407 - acc: 0.1875\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 1.7548 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.6611 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 705us/step - loss: 1.7334 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.4363 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.7300 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.5291 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.8436 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.7590 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.4507 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 706us/step - loss: 1.7512 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.6449 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.8217 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.7308 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.7835 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 1.5629 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 1.6738 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.5988 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 672us/step - loss: 1.6803 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 737us/step - loss: 1.6149 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.8977 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.8322 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.5858 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.3747 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.4803 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.8957 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.5178 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.5523 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 688us/step - loss: 1.4448 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.9652 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 704us/step - loss: 1.9423 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 686us/step - loss: 1.5170 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.5555 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 656us/step - loss: 1.6303 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.5193 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.9061 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8558 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.7856 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.3817 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 700us/step - loss: 1.4289 - acc: 0.5625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.8128 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.6918 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9836 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 676us/step - loss: 1.7936 - acc: 0.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.7679 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.6059 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8574 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 681us/step - loss: 1.8880 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.4777 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9189 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 689us/step - loss: 1.5946 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 695us/step - loss: 1.7707 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.5907 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 787us/step - loss: 1.6334 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 770us/step - loss: 1.7719 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 844us/step - loss: 1.7407 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 747us/step - loss: 1.2665 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.7082 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.4832 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.6952 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.7424 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.4811 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 1.5997 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.9693 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.6307 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 1.5817 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.9331 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 718us/step - loss: 1.7367 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.6431 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.9650 - acc: 0.2188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 1.5080 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 1.4636 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.7598 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.5973 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 717us/step - loss: 1.6722 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 696us/step - loss: 1.6079 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 683us/step - loss: 1.7755 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 691us/step - loss: 1.7544 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 1.8760 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 670us/step - loss: 1.6740 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.6411 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.4778 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 690us/step - loss: 1.4374 - acc: 0.5312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 702us/step - loss: 2.1426 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 1.5692 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.4406 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.6160 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.5923 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 835us/step - loss: 1.7571 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 827us/step - loss: 1.6887 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 828us/step - loss: 1.5525 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 862us/step - loss: 1.5912 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 857us/step - loss: 2.0140 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 733us/step - loss: 1.7950 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.7780 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.9402 - acc: 0.1562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.5975 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 727us/step - loss: 1.7433 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.7857 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 677us/step - loss: 1.7423 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.5897 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 687us/step - loss: 1.8766 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 682us/step - loss: 1.9087 - acc: 0.2500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 699us/step - loss: 1.7856 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 693us/step - loss: 1.4517 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.4850 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 692us/step - loss: 1.8506 - acc: 0.2812\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.8138 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 685us/step - loss: 1.5490 - acc: 0.4375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 708us/step - loss: 1.6339 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.5514 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 684us/step - loss: 1.6142 - acc: 0.3750\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 703us/step - loss: 1.5342 - acc: 0.4688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 698us/step - loss: 1.6187 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 701us/step - loss: 1.7459 - acc: 0.3125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 694us/step - loss: 1.4541 - acc: 0.5625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 680us/step - loss: 1.7157 - acc: 0.4062\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 697us/step - loss: 1.7169 - acc: 0.3438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 0s 708us/step - loss: 1.5344 - acc: 0.4062\n"
     ]
    }
   ],
   "source": [
    "first_model = train_mnist(DEFAULT_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "10000/10000 [==============================] - 2s 201us/step\n",
      "Model evaluation results: {'loss': 1.161294492149353, 'acc': 0.7583}\n"
     ]
    }
   ],
   "source": [
    "evaluate(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now quickly try out this model to see if it works as expected. We'll load the model with our trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try to write a digit into the box below. This will automatically save your input in a variable `data` behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/Javascript\">\n",
       "    var pixels = [];\n",
       "    for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    var click = 0;\n",
       "\n",
       "    var canvas = document.querySelector(\"canvas\");\n",
       "    canvas.addEventListener(\"mousemove\", function(e){\n",
       "        if (e.buttons == 1) {\n",
       "            click = 1;\n",
       "            canvas.getContext(\"2d\").fillStyle = \"rgb(0,0,0)\";\n",
       "            canvas.getContext(\"2d\").fillRect(e.offsetX, e.offsetY, 8, 8);\n",
       "            x = Math.floor(e.offsetY * 0.2);\n",
       "            y = Math.floor(e.offsetX * 0.2) + 1;\n",
       "            for (var dy = 0; dy < 2; dy++){\n",
       "                for (var dx = 0; dx < 2; dx++){\n",
       "                    if ((x + dx < 28) && (y + dy < 28)){\n",
       "                        pixels[(y+dy)+(x+dx)*28] = 1;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        } else {\n",
       "            if (click == 1) set_value();\n",
       "            click = 0;\n",
       "        }\n",
       "    });\n",
       "    function clear_value(){\n",
       "        canvas.getContext(\"2d\").fillStyle = \"rgb(255,255,255)\";\n",
       "        canvas.getContext(\"2d\").fillRect(0, 0, 140, 140);\n",
       "        for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    }\n",
       "    \n",
       "    function set_value(){\n",
       "        var result = \"[[\"\n",
       "        for (var i = 0; i < 28; i++) {\n",
       "            result += \"[\"\n",
       "            for (var j = 0; j < 28; j++) {\n",
       "                result += pixels [i * 28 + j]\n",
       "                if (j < 27) {\n",
       "                    result += \", \"\n",
       "                }\n",
       "            }\n",
       "            result += \"]\"\n",
       "            if (i < 27) {\n",
       "                result += \", \"\n",
       "            }\n",
       "        }\n",
       "        result += \"]]\"\n",
       "        var kernel = IPython.notebook.kernel;\n",
       "        kernel.execute(\"data = \" + result)\n",
       "    }\n",
       "</script>\n",
       "<table>\n",
       "<td style=\"border-style: none;\">\n",
       "<div style=\"border: solid 2px #666; width: 143px; height: 144px;\">\n",
       "<canvas width=\"140\" height=\"140\"></canvas>\n",
       "</div></td>\n",
       "<td style=\"border-style: none;\">\n",
       "<button onclick=\"clear_value()\">Clear</button>\n",
       "</td>\n",
       "</table>\n",
       "\n",
       "<!-- This work has been modified from the original and is licensed under the Apache 2.0 License. -->\n",
       "\n",
       "<!--\n",
       "                                     Apache License\n",
       "                           Version 2.0, January 2004\n",
       "                        http://www.apache.org/licenses/\n",
       "\n",
       "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
       "\n",
       "   1. Definitions.\n",
       "\n",
       "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
       "      and distribution as defined by Sections 1 through 9 of this document.\n",
       "\n",
       "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
       "      the copyright owner that is granting the License.\n",
       "\n",
       "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
       "      other entities that control, are controlled by, or are under common\n",
       "      control with that entity. For the purposes of this definition,\n",
       "      \"control\" means (i) the power, direct or indirect, to cause the\n",
       "      direction or management of such entity, whether by contract or\n",
       "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
       "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
       "\n",
       "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
       "      exercising permissions granted by this License.\n",
       "\n",
       "      \"Source\" form shall mean the preferred form for making modifications,\n",
       "      including but not limited to software source code, documentation\n",
       "      source, and configuration files.\n",
       "\n",
       "      \"Object\" form shall mean any form resulting from mechanical\n",
       "      transformation or translation of a Source form, including but\n",
       "      not limited to compiled object code, generated documentation,\n",
       "      and conversions to other media types.\n",
       "\n",
       "      \"Work\" shall mean the work of authorship, whether in Source or\n",
       "      Object form, made available under the License, as indicated by a\n",
       "      copyright notice that is included in or attached to the work\n",
       "      (an example is provided in the Appendix below).\n",
       "\n",
       "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
       "      form, that is based on (or derived from) the Work and for which the\n",
       "      editorial revisions, annotations, elaborations, or other modifications\n",
       "      represent, as a whole, an original work of authorship. For the purposes\n",
       "      of this License, Derivative Works shall not include works that remain\n",
       "      separable from, or merely link (or bind by name) to the interfaces of,\n",
       "      the Work and Derivative Works thereof.\n",
       "\n",
       "      \"Contribution\" shall mean any work of authorship, including\n",
       "      the original version of the Work and any modifications or additions\n",
       "      to that Work or Derivative Works thereof, that is intentionally\n",
       "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
       "      or by an individual or Legal Entity authorized to submit on behalf of\n",
       "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
       "      means any form of electronic, verbal, or written communication sent\n",
       "      to the Licensor or its representatives, including but not limited to\n",
       "      communication on electronic mailing lists, source code control systems,\n",
       "      and issue tracking systems that are managed by, or on behalf of, the\n",
       "      Licensor for the purpose of discussing and improving the Work, but\n",
       "      excluding communication that is conspicuously marked or otherwise\n",
       "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
       "\n",
       "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
       "      on behalf of whom a Contribution has been received by Licensor and\n",
       "      subsequently incorporated within the Work.\n",
       "\n",
       "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      copyright license to reproduce, prepare Derivative Works of,\n",
       "      publicly display, publicly perform, sublicense, and distribute the\n",
       "      Work and such Derivative Works in Source or Object form.\n",
       "\n",
       "   3. Grant of Patent License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      (except as stated in this section) patent license to make, have made,\n",
       "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
       "      where such license applies only to those patent claims licensable\n",
       "      by such Contributor that are necessarily infringed by their\n",
       "      Contribution(s) alone or by combination of their Contribution(s)\n",
       "      with the Work to which such Contribution(s) was submitted. If You\n",
       "      institute patent litigation against any entity (including a\n",
       "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
       "      or a Contribution incorporated within the Work constitutes direct\n",
       "      or contributory patent infringement, then any patent licenses\n",
       "      granted to You under this License for that Work shall terminate\n",
       "      as of the date such litigation is filed.\n",
       "\n",
       "   4. Redistribution. You may reproduce and distribute copies of the\n",
       "      Work or Derivative Works thereof in any medium, with or without\n",
       "      modifications, and in Source or Object form, provided that You\n",
       "      meet the following conditions:\n",
       "\n",
       "      (a) You must give any other recipients of the Work or\n",
       "          Derivative Works a copy of this License; and\n",
       "\n",
       "      (b) You must cause any modified files to carry prominent notices\n",
       "          stating that You changed the files; and\n",
       "\n",
       "      (c) You must retain, in the Source form of any Derivative Works\n",
       "          that You distribute, all copyright, patent, trademark, and\n",
       "          attribution notices from the Source form of the Work,\n",
       "          excluding those notices that do not pertain to any part of\n",
       "          the Derivative Works; and\n",
       "\n",
       "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
       "          distribution, then any Derivative Works that You distribute must\n",
       "          include a readable copy of the attribution notices contained\n",
       "          within such NOTICE file, excluding those notices that do not\n",
       "          pertain to any part of the Derivative Works, in at least one\n",
       "          of the following places: within a NOTICE text file distributed\n",
       "          as part of the Derivative Works; within the Source form or\n",
       "          documentation, if provided along with the Derivative Works; or,\n",
       "          within a display generated by the Derivative Works, if and\n",
       "          wherever such third-party notices normally appear. The contents\n",
       "          of the NOTICE file are for informational purposes only and\n",
       "          do not modify the License. You may add Your own attribution\n",
       "          notices within Derivative Works that You distribute, alongside\n",
       "          or as an addendum to the NOTICE text from the Work, provided\n",
       "          that such additional attribution notices cannot be construed\n",
       "          as modifying the License.\n",
       "\n",
       "      You may add Your own copyright statement to Your modifications and\n",
       "      may provide additional or different license terms and conditions\n",
       "      for use, reproduction, or distribution of Your modifications, or\n",
       "      for any such Derivative Works as a whole, provided Your use,\n",
       "      reproduction, and distribution of the Work otherwise complies with\n",
       "      the conditions stated in this License.\n",
       "\n",
       "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
       "      any Contribution intentionally submitted for inclusion in the Work\n",
       "      by You to the Licensor shall be under the terms and conditions of\n",
       "      this License, without any additional terms or conditions.\n",
       "      Notwithstanding the above, nothing herein shall supersede or modify\n",
       "      the terms of any separate license agreement you may have executed\n",
       "      with Licensor regarding such Contributions.\n",
       "\n",
       "   6. Trademarks. This License does not grant permission to use the trade\n",
       "      names, trademarks, service marks, or product names of the Licensor,\n",
       "      except as required for reasonable and customary use in describing the\n",
       "      origin of the Work and reproducing the content of the NOTICE file.\n",
       "\n",
       "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
       "      agreed to in writing, Licensor provides the Work (and each\n",
       "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
       "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
       "      implied, including, without limitation, any warranties or conditions\n",
       "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
       "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
       "      appropriateness of using or redistributing the Work and assume any\n",
       "      risks associated with Your exercise of permissions under this License.\n",
       "\n",
       "   8. Limitation of Liability. In no event and under no legal theory,\n",
       "      whether in tort (including negligence), contract, or otherwise,\n",
       "      unless required by applicable law (such as deliberate and grossly\n",
       "      negligent acts) or agreed to in writing, shall any Contributor be\n",
       "      liable to You for damages, including any direct, indirect, special,\n",
       "      incidental, or consequential damages of any character arising as a\n",
       "      result of this License or out of the use or inability to use the\n",
       "      Work (including but not limited to damages for loss of goodwill,\n",
       "      work stoppage, computer failure or malfunction, or any and all\n",
       "      other commercial damages or losses), even if such Contributor\n",
       "      has been advised of the possibility of such damages.\n",
       "\n",
       "   9. Accepting Warranty or Additional Liability. While redistributing\n",
       "      the Work or Derivative Works thereof, You may choose to offer,\n",
       "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
       "      or other liability obligations and/or rights consistent with this\n",
       "      License. However, in accepting such obligations, You may act only\n",
       "      on Your own behalf and on Your sole responsibility, not on behalf\n",
       "      of any other Contributor, and only if You agree to indemnify,\n",
       "      defend, and hold each Contributor harmless for any liability\n",
       "      incurred by, or claims asserted against, such Contributor by reason\n",
       "      of your accepting any such warranty or additional liability.\n",
       "\n",
       "   END OF TERMS AND CONDITIONS\n",
       "\n",
       "   APPENDIX: How to apply the Apache License to your work.\n",
       "\n",
       "      To apply the Apache License to your work, attach the following\n",
       "      boilerplate notice, with the fields enclosed by brackets \"{}\"\n",
       "      replaced with your own identifying information. (Don't include\n",
       "      the brackets!)  The text should be enclosed in the appropriate\n",
       "      comment syntax for the file format. We also recommend that a\n",
       "      file or class name and description of purpose be included on the\n",
       "      same \"printed page\" as the copyright notice for easier\n",
       "      identification within third-party archives.\n",
       "\n",
       "   Copyright {yyyy} {name of copyright owner}\n",
       "\n",
       "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "   you may not use this file except in compliance with the License.\n",
       "   You may obtain a copy of the License at\n",
       "\n",
       "       http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "   Unless required by applicable law or agreed to in writing, software\n",
       "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "   See the License for the specific language governing permissions and\n",
       "   limitations under the License.\n",
       "-->\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = None\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tip: don't expect it to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model predicted your input as 6\n"
     ]
    }
   ],
   "source": [
    "prepared_data = prepare_data(data)\n",
    "print(\"This model predicted your input as\", first_model.predict(prepared_data).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've now set up a model that we can use Tune to optimize!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up Tune\n",
    "\n",
    "One thing we might want to do now is find better hyperparameters so that our model trains more quickly and possibly to a higher accuracy. Let's make some minor modifications to utilize Tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune uses Ray as a backend, so we will first import and initialize Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-01-24_17-56-57_29787/logs.\n",
      "Waiting for redis server at 127.0.0.1:33249 to respond...\n",
      "Waiting for redis server at 127.0.0.1:35764 to respond...\n",
      "Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=cd7e1fb14de2294362af92e0164e4060f1e860655578f72b\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.23.45',\n",
       " 'redis_address': '192.168.23.45:33249',\n",
       " 'object_store_addresses': ['/tmp/ray/session_2019-01-24_17-56-57_29787/sockets/plasma_store'],\n",
       " 'raylet_socket_names': ['/tmp/ray/session_2019-01-24_17-56-57_29787/sockets/raylet'],\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui.ipynb?token=cd7e1fb14de2294362af92e0164e4060f1e860655578f72b'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune will automate and distribute your hyperparameter search by scheduling a number of **trials** on a machine. Each trial runs a user-defined Python function with a sampled set of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Two steps to use Tune:\n",
    "\n",
    "Step 1) For the function you wish to tune, we need to change the signature to a specific format as shown below. Specifically: **pass in a ``reporter`` object to the below `train_mnist_tune` class**.\n",
    "\n",
    "```python\n",
    "def trainable(config, reporter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        config (dict): Parameters provided from the search algorithm\n",
    "            or variant generation.\n",
    "        reporter (Reporter): Handle to report intermediate metrics to Tune.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Step 2) We want to keep track of performance as the model is training. Specifically: **get the `mean_accuracy` from Keras, and call the ``reporter`` to report the `mean_accuracy` for every batch**. \n",
    "\n",
    "You can get model accuracy from Keras with the following code:\n",
    "\n",
    "```python\n",
    "mean_accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "```\n",
    "\n",
    "\n",
    "Example of using the reporter:\n",
    "\n",
    "```python\n",
    "def train_func(config, reporter):  # add a reporter arg\n",
    "    # ...\n",
    "    for data, target in dataset:\n",
    "        model.fit(data, target)\n",
    "        # save model here\n",
    "        accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "        reporter(mean_accuracy=accuracy) # report metrics\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_tune(config, reporter): ### TODO: Change this function signature following step 1 #####\n",
    "    data_generator = load_data()\n",
    "    model = make_model(config)\n",
    "    for i, (x_batch, y_batch) in enumerate(data_generator):\n",
    "        model.fit(x_batch, y_batch, verbose=0)\n",
    "        if i % 3 == 0:\n",
    "            last_checkpoint = \"weights_tune_{}.h5\".format(i)\n",
    "            model.save_weights(last_checkpoint)\n",
    "        ### Don't change above ############### \n",
    "        mean_accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "        reporter(mean_accuracy=result[1], timesteps_total=i, checkpoint=last_checkpoint)\n",
    "        ### TODO: Use the reporter here to fill out intermediate metrics following step 2###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4567fc9a242c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This may take 30 seconds or so to run if incorrectly written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtest_reporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mnist_tune\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/pccfs/backed_up/jaredtn/research/ray/tutorial/tune_exercises/helper.py\u001b[0m in \u001b[0;36mtest_reporter\u001b[0;34m(train_mnist_tune)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGoodError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This works.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtrain_mnist_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmock_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Forgot to modify function signature?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-110d0c1abacf>\u001b[0m in \u001b[0;36mtrain_mnist_tune\u001b[0;34m(config, reporter)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m### Don't change above ###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmean_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mreporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m### TODO: Use the reporter here to fill out intermediate metrics following step 2###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# This may take 30 seconds or so to run if incorrectly written\n",
    "assert test_reporter(train_mnist_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you've done this correctly, you now have properly converted your function to be Tune-compatible!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Let's now try to search over some parameters. \n",
    "\n",
    "*NOTE: You can find the documentation for this section here: https://ray.readthedocs.io/en/latest/tune-usage.html#specifying-experiments*\n",
    "\n",
    "In this section, we'll use some basic Tune features for training - namely specifying a stopping criteria and a search space. \n",
    "\n",
    "Let's first create a Tune Experiment specification. The relevant documentation for the Experiment class is here:\n",
    "\n",
    "```python\n",
    "class ray.tune.Experiment(name, run, stop=None, config=None, ... ):\n",
    "    \"\"\"Tracks experiment specifications.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Name of experiment.\n",
    "        run (function|class|str): The algorithm or model to train.\n",
    "            This may refer to the name of a built-on algorithm\n",
    "            (e.g. RLLib's DQN or PPO), a user-defined trainable\n",
    "            function or class, or the string identifier of a\n",
    "            trainable function or class registered in the tune registry.\n",
    "        stop (dict): The stopping criteria. The keys may be any field in\n",
    "            the return result of 'train()', whichever is reached first.\n",
    "            Defaults to empty dict.\n",
    "        config (dict): Algorithm-specific configuration for Tune variant\n",
    "            generation (e.g. env, hyperparams). Defaults to empty dict.\n",
    "            Custom search algorithms may ignore this.\n",
    "        trial_resources (dict): Machine resources to allocate per trial,\n",
    "            e.g. ``{\"cpu\": 64, \"gpu\": 8}``. Note that GPUs will not be\n",
    "            assigned unless you specify them here. Defaults to 1 CPU and 0\n",
    "            GPUs in ``Trainable.default_resource_request()``.\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**: First, **set the stopping criteria to when `mean_accuracy` passes `0.95`**. For example, to specify that trials will be stopped whenever they report `arbitrary_metric` that is `>= 500`, do:\n",
    "\n",
    "```python\n",
    "stop={\"arbitrary_metric\": 500}\n",
    "```\n",
    "\n",
    "\n",
    "**Part 2**: We also want to designate a search space. We'll search over *learning rate*, which sets the step size of our model update, and *momentum*, which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "\n",
    "You can use `tune.grid_search` to specify an axis of a grid search. By default, Tune also supports sampling parameters from user-specified lambda functions, which can be used independently or in combination with grid search.  The following example shows grid search over a set of values combined with random sampling from a lambda functions, generating 3 different trials. \n",
    "\n",
    "```python\n",
    "configuration = tune.Experiment(\n",
    "    # ...\n",
    "    config={\n",
    "        \"arbitrary_parameter1\": lambda spec: np.random.uniform(0.1, 100),\n",
    "        \"arbitrary_parameter2\": tune.grid_search([16, 64, 256]),\n",
    "        # ...\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Specifically, \n",
    "1. randomly search for learning rate `\"lr\"` between 0.001 to 0.1,\n",
    "2. do a grid search over `\"momentum\"` for `[0.2, 0.4, 0.6]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_mnist_tune,\n",
    "    trial_resources={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 0.95},  # TODO: Part 1\n",
    "    config={\"lr\": lambda spec: np.random.uniform(0.001, 0.1),\n",
    "            \"momentum\": tune.grid_search([0.2, 0.4, 0.6])}  # TODO: Part 2\n",
    ")\n",
    "\n",
    "assert configuration.spec.get(\"stop\", {}).get(\"mean_accuracy\") == 0.95\n",
    "assert \"grid_search\" in configuration.spec.get(\"config\", {}).get(\"momentum\", {})\n",
    "assert \"lr\" in configuration.spec.get(\"config\", {})\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run our experiment with a single line of code. \n",
    "\n",
    "*Note*: Be sure pay attention to the `acc` metric next to each running trial. That indicates the most recently reported mean accuracy for that trial. This should evaluate in less than a minute. The output will look something similar to:\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/8 CPUs, 0/1 GPUs\n",
    "Result logdir: .../ray_results/experiment_name\n",
    "RUNNING trials:\n",
    " - train_mnist_tune_0_lr=0.085836,momentum=0.2:\tRUNNING [pid=44320], 4 s, 3 iter, 0.406 acc\n",
    " - train_mnist_tune_1_lr=0.062562,momentum=0.4:\tRUNNING [pid=44321], 3 s, 2 iter, 0.219 acc\n",
    " - train_mnist_tune_2_lr=0.099461,momentum=0.6:\tRUNNING [pid=44317], 3 s, 2 iter, 0.281 acc\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.stdout\n",
    "trials = tune.run_experiments(configuration, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect the result below to be about `0.6`, although your mileage may vary (and it's OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best result is\", get_best_result(trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've run your first Tune experiment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try using a scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Tune schedules trials in serial order with the `FIFOScheduler` class. However, you can also specify a custom scheduling algorithm that can early stop trials or perturb parameters. \n",
    "\n",
    "Let's use a state of the art algorithm, `HyperBand`, to scale up and accelerate our training. Hyperband is an algorithm that focuses on speeding up random search through adaptive resource allocation and early-stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**: Let's scale up our search. \n",
    "\n",
    "1) Sample the search space 5 times. (https://ray.readthedocs.io/en/latest/tune-usage.html#sampling-multiple-times).\n",
    "\n",
    "2) Search over another hyperparameter: `\"hidden\"` from 16 to 512 which specifies the size of the last neural network layer.\n",
    "\n",
    "Here, use `np.random.randint`. \n",
    "```python\n",
    "numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "    \"\"\"Return random integers from low (inclusive) to high (exclusive).\"\"\"\n",
    "```\n",
    "\n",
    "An extended version of the `Experiment` documentation is shown below. Note that you should expect a total of 15 trials, due to the usage of `grid_search`.\n",
    "\n",
    "```python\n",
    "class ray.tune.Experiment(name, run, stop=None, config=None, ... ):\n",
    "    \"\"\"Tracks experiment specifications.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Name of experiment.\n",
    "        run (function|class|str): The algorithm or model to train.\n",
    "            This may refer to the name of a built-on algorithm\n",
    "            (e.g. RLLib's DQN or PPO), a user-defined trainable\n",
    "            function or class, or the string identifier of a\n",
    "            trainable function or class registered in the tune registry.\n",
    "        stop (dict): The stopping criteria. The keys may be any field in\n",
    "            the return result of 'train()', whichever is reached first.\n",
    "            Defaults to empty dict.\n",
    "        config (dict): Algorithm-specific configuration for Tune variant\n",
    "            generation (e.g. env, hyperparams). Defaults to empty dict.\n",
    "            Custom search algorithms may ignore this.\n",
    "        trial_resources (dict): Machine resources to allocate per trial,\n",
    "            e.g. ``{\"cpu\": 64, \"gpu\": 8}``. Note that GPUs will not be\n",
    "            assigned unless you specify them here. Defaults to 1 CPU and 0\n",
    "            GPUs in ``Trainable.default_resource_request()``.\n",
    "        num_samples (int): Number of times to sample from the\n",
    "            hyperparameter space. Defaults to 1. If `grid_search` is\n",
    "            provided as an argument, the grid will be repeated\n",
    "            `num_samples` of times.\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration2 = tune.Experiment(\n",
    "    \"experiment2\",\n",
    "    run=train_mnist_tune,\n",
    "    num_samples=5, ## TODO: Change this to 5\n",
    "    trial_resources={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 0.95},\n",
    "    config={\n",
    "        \"lr\": lambda spec: np.random.uniform(0.001, 0.1),\n",
    "        \"momentum\": tune.grid_search([0.2, 0.4, 0.6]),\n",
    "        \"hidden\": lambda spec: np.random.randint(16, high=513), ## TODO: Sample uniformly from 16 to 512\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**: Create an Asynchronous HyperBand Scheduler (https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband). The documentation is shown below. \n",
    "\n",
    "Be sure to set the `time_attr` to `timesteps_total` and `reward_attr` to `mean_accuracy`.\n",
    "\n",
    "```python\n",
    "class AsyncHyperBandScheduler(FIFOScheduler):\n",
    "    \"\"\"This provides HyperBand functionality to your search.\n",
    "    \n",
    "    Hyperband is an algorithm that focuses on speeding up \n",
    "    random search through adaptive resource allocation and early-stopping.\n",
    "\n",
    "    See https://openreview.net/forum?id=S1Y7OOlRZ\n",
    "\n",
    "    Args:\n",
    "        time_attr (str): A training result attr to use for comparing time.\n",
    "            Note that you can pass in something non-temporal such as\n",
    "            `training_iteration` as a measure of progress, the only requirement\n",
    "            is that the attribute should increase monotonically.\n",
    "        reward_attr (str): The training result objective value attribute. As\n",
    "            with `time_attr`, this may refer to any objective value. Stopping\n",
    "            procedures will use this attribute.\n",
    "        ...\n",
    "        \n",
    "    Examples:\n",
    "        >>> hyperband = AsyncHyperBandScheduler(\n",
    "        >>>     time_attr='timesteps_total',\n",
    "        >>>     reward_attr='mean_accuracy')\n",
    "    \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "## TODO: Create an Asynchronous HyperBand Scheduler\n",
    "hyperband = AsyncHyperBandScheduler(\n",
    "    time_attr='timesteps_total',\n",
    "    reward_attr='mean_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous configuration, pass in the HyperBand scheduler to `run_experiments`.\n",
    "\n",
    "Recall that the `run_experiments` API is:\n",
    "```python\n",
    "def run_experiments(experiments=None,\n",
    "                    search_alg=None,\n",
    "                    scheduler=None,\n",
    "                    ...):\n",
    "    \"\"\"Runs and blocks until all trials finish.\n",
    "\n",
    "    Args:\n",
    "        experiments (Experiment | list | dict): Experiments to run. Will be\n",
    "            passed to `search_alg` via `add_configurations`.\n",
    "        search_alg (SearchAlgorithm): Search Algorithm. Defaults to\n",
    "            BasicVariantGenerator.\n",
    "        scheduler (TrialScheduler): Scheduler for executing\n",
    "            the experiment. Choose among FIFO (default), MedianStopping,\n",
    "            AsyncHyperBand, and HyperBand.\n",
    "        ...\n",
    "    \n",
    "    Returns:\n",
    "        List of Trial objects, holding data for each executed trial.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Call `run_experiments` with the correct arguments. This may take multiple minutes for the experiment to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you should be using `configuration2` instead of `configuration`.\n",
    "trials = tune.run_experiments(configuration2, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now, let's get the best model from your search process, and check its validation accuracy compared to the first model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best model...\")\n",
    "evaluate(best_model)\n",
    "print(\"First model...\")\n",
    "evaluate(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your model on some manual inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write a digit into the canvas below. This will automatically load your input into the variable `final_data`. (Due to randomness, your mileage may vary. If your model is bad, try increasing the number of samples with `load_data(num_samples=X)` in the above defined trainables, or try the Optional Search Algorithm section below).\n",
    "\n",
    "Typically, the model doesn't work quite well on 1 and 8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(open(\"input_final.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_input = prepare_data(final_data)\n",
    "best = best_model.predict(manual_input).argmax()\n",
    "first = first_model.predict(manual_input).argmax()\n",
    "\n",
    "print(\"Best model got {}, first model got {}\".format(best, first))\n",
    "plt.imshow(manual_input.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you're now a Tune expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please: fill out this form to provide feedback on this tutorial!\n",
    "\n",
    "https://goo.gl/forms/NVTFjUKFz4TH8kgK2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Try using a search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is an execution layer, so we can combine powerful optimizers such as HyperOpt (https://github.com/hyperopt/hyperopt) with state-of-the-art algorithms such as HyperBand without modifying any model training code.\n",
    "\n",
    "The documentation to doing this is here: https://ray.readthedocs.io/en/latest/tune-searchalg.html#hyperopt-search-tree-structured-parzen-estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "    \"hidden\": hp.choice(\"hidden\", np.arange(16, 256, dtype=int)),\n",
    "}\n",
    "\n",
    "## TODO: CREATE A HyperOptObject\n",
    "hyperopt_search = HyperOptSearch(space, reward_attr=\"mean_accuracy\")\n",
    "\n",
    "## TODO: Pass in the object to Tune.\n",
    "good_results = tune.run_experiments(\n",
    "    configuration2, search_alg=hyperopt_search, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to compare your best model here\n",
    "best_model2 = get_best_model(make_model, good_results, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the canvas data from above \n",
    "\n",
    "manual_input = prepare_data(final_data)\n",
    "best = best_model.predict(manual_input).argmax()\n",
    "best2 = best_model2.predict(manual_input).argmax()\n",
    "first = first_model.predict(manual_input).argmax()\n",
    "\n",
    "print(\"Best model got {}, HyperOpt model got {}, first model got {}\".format(best, best2, first))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
