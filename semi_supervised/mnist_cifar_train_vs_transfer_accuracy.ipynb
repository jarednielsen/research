{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning of MNIST CNN\n",
    "Jared Nielsen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results**\n",
    "\n",
    "\n",
    "### MNIST\n",
    "`model_B` achieves 90.6% accuracy on `data_A` when `model_A` has 89.9% accuracy on `data_A`.  \n",
    "`model_B` achieves 96.6% accuracy on `data_A` when `model_A` has 97.9% accuracy on `data_A`.\n",
    "\n",
    "### Fashion-MNIST\n",
    "`model_B` achieves 81.1% accuracy on `data_A` when `model_A` has 83.5% accuracy on `data_A`.  \n",
    "`model_B` achieves 85.5% accuracy on `data_A` when `model_A` has 87.6% accuracy on `data_A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notes**\n",
    "\n",
    "### Terminology\n",
    "`acc*` - The training accuracy when `model_A` is trained on all the labeled data.  \n",
    "`acc1` - `model_A` on `data_A`  \n",
    "`acc2` - `model_A` on `data_B`  \n",
    "`acc3` - `model_B` on `data_A`  \n",
    "`acc4` - `model_B` on `data_B`  \n",
    "`acc5` - `model_B` on `data_B_hat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mag.experiment import Experiment\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from time import sleep\n",
    "import ray\n",
    "\n",
    "# import mnist_cnn\n",
    "# from mnist_cnn import Net, model_A_path, model_B_path\n",
    "\n",
    "import cifar_cnn\n",
    "from cifar_cnn import Net, model_A_path, model_B_path\n",
    "\n",
    "# tqdm_disable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `dataset_A` and `dataset_B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 1e-3 #0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using cuda')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('using cpu')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# root_dir = \"../data/fashion-mnist/\"\n",
    "# mnist_train = torchvision.datasets.FashionMNIST(root_dir, train=True, download=True,\n",
    "#                              transform=torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                torchvision.transforms.Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ]))\n",
    "\n",
    "root_dir = \"../data/cifar10/\"\n",
    "data_train = torchvision.datasets.CIFAR10(root_dir, train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                             ]))\n",
    "len_data = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train `model_A` on `dataset_A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_A: 25000, data_B: 25000\n",
      "\n",
      "Test set: Avg. loss: 1.6385, Accuracy: 9665/25000 (38.66%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6503, Accuracy: 9419/25000 (37.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5032, Accuracy: 11190/25000 (44.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5331, Accuracy: 10918/25000 (43.67%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e6d9644e134b0ab802441fa02b1e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='model_B', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.2205, Accuracy: 9966/25000 (39.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2300, Accuracy: 9876/25000 (39.50%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4566, Accuracy: 10206/25000 (40.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4851, Accuracy: 10137/25000 (40.55%)\n",
      "\n",
      "\n",
      "([38.66, 44.76], [37.676, 43.672], [39.864, 40.824], [39.504, 40.548])\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def train(model_A, optimizer_A, epoch, dataloader):\n",
    "    model_A.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer_A.zero_grad()\n",
    "        output = model_A(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_A.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            torch.save(model_A.state_dict(), model_A_path)\n",
    "\n",
    "def test(network, dataloader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        test_loss /= len(dataloader.dataset)\n",
    "        acc = 100 * correct.item() / len(dataloader.dataset)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(dataloader.dataset), acc))\n",
    "        \n",
    "    return acc, test_loss\n",
    "\n",
    "def train_model_A(model_A, opt_A, dataloader, n_epochs, test=False):  \n",
    "    acc1s, acc2s = [], []\n",
    "    for i_epoch in range(n_epochs):\n",
    "        train(model_A=model_A, optimizer_A=opt_A, epoch=i_epoch,\n",
    "              dataloader=dataloader)\n",
    "        \n",
    "def train_with_transfer_labels(model_A, model_B, optimizer_B, epoch, dataloader):\n",
    "    model_A.eval()\n",
    "    model_B.train()\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target_hat = model_A(data)\n",
    "        target_hat = torch.argmax(target_hat, dim=1)\n",
    "        optimizer_B.zero_grad()\n",
    "        output = model_B(data)\n",
    "        loss_hat = F.nll_loss(output, target_hat)\n",
    "        loss_hat.backward()\n",
    "        optimizer_B.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            torch.save(model_B.state_dict(), model_B_path)\n",
    "            \n",
    "def train_model_B(model_A, model_B, opt_B, dataloader, n_epochs):\n",
    "    pbar = tqdm_notebook(range(n_epochs), desc='model_A', total=n_epochs)\n",
    "    for i_epoch in pbar:\n",
    "        train_with_transfer_labels(model_A=model_A, model_B=model_B, optimizer_B=opt_B,\n",
    "                                  epoch=i_epoch, dataloader=dataloader)\n",
    "\n",
    "# @ray.remote\n",
    "def get_reverse_accuracy(pct, n_epochs):\n",
    "    train_size = int(len_data * pct)\n",
    "    test_size = len_data - train_size\n",
    "    dataset_A, dataset_B = random_split(data_train, [train_size, test_size])\n",
    "    print(\"data_A: {}, data_B: {}\".format(len(dataset_A), len(dataset_B)))\n",
    "    loader_A, loader_B = [DataLoader(dataset, batch_size=batch_size_train, shuffle=True) \n",
    "                          for dataset in (dataset_A, dataset_B)]\n",
    "    \n",
    "    model_A = Net().to(device)\n",
    "    opt_A = optim.Adam(model_A.parameters(), lr=learning_rate)\n",
    "    \n",
    "    acc1s, acc2s = [], []\n",
    "    for i_epoch in range(n_epochs):\n",
    "        train(model_A=model_A, optimizer_A=opt_A, epoch=i_epoch,\n",
    "              dataloader=loader_A)\n",
    "        acc1, loss = test(network=model_A, dataloader=loader_A)\n",
    "        acc2, loss = test(network=model_A, dataloader=loader_B)\n",
    "        acc1s.append(acc1)\n",
    "        acc2s.append(acc2)\n",
    "        \n",
    "#     train_model_A(model_A=model_A, opt_A=opt_A, dataloader=loader_A, n_epochs=n_epochs)\n",
    "    \n",
    "    model_B = Net().to(device)\n",
    "    opt_B = optim.Adam(model_B.parameters(), lr=learning_rate)\n",
    "    \n",
    "    acc3s, acc4s = [], []\n",
    "    pbar = tqdm_notebook(range(n_epochs), desc='model_B', total=n_epochs)\n",
    "    for i_epoch in pbar:\n",
    "        train_with_transfer_labels(model_A=model_A, model_B=model_B, optimizer_B=opt_B,\n",
    "                                  epoch=i_epoch, dataloader=loader_B)\n",
    "        acc3, loss = test(network=model_B, dataloader=loader_A)\n",
    "        acc4, loss = test(network=model_B, dataloader=loader_B)\n",
    "        acc3s.append(acc3)\n",
    "        acc4s.append(acc4)\n",
    "        \n",
    "#     train_model_B(model_A=model_A, model_B=model_B, opt_B=opt_B,\n",
    "#                  dataloader=loader_B, n_epochs=n_epochs)\n",
    "    \n",
    "#     acc1, loss = test(network=model_A, dataloader=loader_A)\n",
    "#     acc2, loss = test(network=model_A, dataloader=loader_B)\n",
    "#     acc3, loss = test(network=model_B, dataloader=loader_A)\n",
    "#     acc4, loss = test(network=model_B, dataloader=loader_B)\n",
    "    return acc1s, acc2s, acc3s, acc4s\n",
    "\n",
    "\n",
    "print(get_reverse_accuracy(pct=0.50, n_epochs=2))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_pct: 0.002\n",
      "pcts: [0.05]\n",
      "experimenting with pct=0.05\n",
      "data_A: 2500, data_B: 47500\n",
      "\n",
      "Test set: Avg. loss: 2.1359, Accuracy: 541/2500 (21.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1299, Accuracy: 10000/47500 (21.05%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9545, Accuracy: 738/2500 (29.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9567, Accuracy: 13265/47500 (27.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8169, Accuracy: 805/2500 (32.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8416, Accuracy: 14186/47500 (29.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7334, Accuracy: 898/2500 (35.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7741, Accuracy: 16256/47500 (34.22%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7061, Accuracy: 891/2500 (35.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7515, Accuracy: 15947/47500 (33.57%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6289, Accuracy: 982/2500 (39.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7071, Accuracy: 17057/47500 (35.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6102, Accuracy: 1005/2500 (40.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7013, Accuracy: 17234/47500 (36.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5347, Accuracy: 1091/2500 (43.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6413, Accuracy: 18398/47500 (38.73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5205, Accuracy: 1132/2500 (45.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6326, Accuracy: 19058/47500 (40.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4749, Accuracy: 1140/2500 (45.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6086, Accuracy: 19429/47500 (40.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4285, Accuracy: 1184/2500 (47.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5911, Accuracy: 19605/47500 (41.27%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4280, Accuracy: 1227/2500 (49.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6149, Accuracy: 19548/47500 (41.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3820, Accuracy: 1249/2500 (49.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5735, Accuracy: 20004/47500 (42.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3643, Accuracy: 1277/2500 (51.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5775, Accuracy: 20012/47500 (42.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3261, Accuracy: 1306/2500 (52.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5747, Accuracy: 20301/47500 (42.74%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3080, Accuracy: 1293/2500 (51.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5666, Accuracy: 20406/47500 (42.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2716, Accuracy: 1361/2500 (54.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5591, Accuracy: 20751/47500 (43.69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2671, Accuracy: 1366/2500 (54.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5638, Accuracy: 20526/47500 (43.21%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2044, Accuracy: 1446/2500 (57.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5398, Accuracy: 21101/47500 (44.42%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1939, Accuracy: 1417/2500 (56.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5423, Accuracy: 20908/47500 (44.02%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1847, Accuracy: 1449/2500 (57.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5618, Accuracy: 20819/47500 (43.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2033, Accuracy: 1443/2500 (57.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6207, Accuracy: 20239/47500 (42.61%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1496, Accuracy: 1478/2500 (59.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5704, Accuracy: 21111/47500 (44.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0975, Accuracy: 1540/2500 (61.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5427, Accuracy: 21345/47500 (44.94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1106, Accuracy: 1507/2500 (60.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5947, Accuracy: 20820/47500 (43.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0666, Accuracy: 1541/2500 (61.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5751, Accuracy: 21177/47500 (44.58%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0100, Accuracy: 1598/2500 (63.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5546, Accuracy: 21395/47500 (45.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9761, Accuracy: 1650/2500 (66.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5690, Accuracy: 21501/47500 (45.27%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0144, Accuracy: 1576/2500 (63.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6032, Accuracy: 21269/47500 (44.78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.9301, Accuracy: 1669/2500 (66.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6130, Accuracy: 21490/47500 (45.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8940, Accuracy: 1751/2500 (70.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5717, Accuracy: 21610/47500 (45.49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8879, Accuracy: 1735/2500 (69.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5869, Accuracy: 21583/47500 (45.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8698, Accuracy: 1735/2500 (69.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6014, Accuracy: 21585/47500 (45.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8418, Accuracy: 1759/2500 (70.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6352, Accuracy: 21860/47500 (46.02%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8174, Accuracy: 1801/2500 (72.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6314, Accuracy: 21511/47500 (45.29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.8417, Accuracy: 1781/2500 (71.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6623, Accuracy: 21329/47500 (44.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7870, Accuracy: 1832/2500 (73.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6726, Accuracy: 21381/47500 (45.01%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7471, Accuracy: 1860/2500 (74.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6939, Accuracy: 21808/47500 (45.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7462, Accuracy: 1875/2500 (75.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6941, Accuracy: 21352/47500 (44.95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.7035, Accuracy: 1938/2500 (77.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6591, Accuracy: 21661/47500 (45.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6980, Accuracy: 1884/2500 (75.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7523, Accuracy: 21228/47500 (44.69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6670, Accuracy: 1948/2500 (77.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7369, Accuracy: 21591/47500 (45.45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6169, Accuracy: 1997/2500 (79.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7359, Accuracy: 21742/47500 (45.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.6154, Accuracy: 1994/2500 (79.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7778, Accuracy: 21469/47500 (45.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5931, Accuracy: 2035/2500 (81.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7525, Accuracy: 21639/47500 (45.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5630, Accuracy: 2052/2500 (82.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8024, Accuracy: 21612/47500 (45.50%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5412, Accuracy: 2080/2500 (83.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8071, Accuracy: 21759/47500 (45.81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5375, Accuracy: 2055/2500 (82.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7994, Accuracy: 21756/47500 (45.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.5203, Accuracy: 2103/2500 (84.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8250, Accuracy: 21649/47500 (45.58%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4803, Accuracy: 2143/2500 (85.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8787, Accuracy: 21683/47500 (45.65%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4650, Accuracy: 2161/2500 (86.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8703, Accuracy: 21648/47500 (45.57%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4499, Accuracy: 2140/2500 (85.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9687, Accuracy: 21534/47500 (45.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4448, Accuracy: 2166/2500 (86.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9719, Accuracy: 21489/47500 (45.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4134, Accuracy: 2195/2500 (87.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9816, Accuracy: 21436/47500 (45.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4015, Accuracy: 2214/2500 (88.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9587, Accuracy: 21697/47500 (45.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.4003, Accuracy: 2232/2500 (89.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9287, Accuracy: 21413/47500 (45.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3737, Accuracy: 2219/2500 (88.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1035, Accuracy: 21747/47500 (45.78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3583, Accuracy: 2256/2500 (90.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0662, Accuracy: 21455/47500 (45.17%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3831, Accuracy: 2235/2500 (89.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1320, Accuracy: 20657/47500 (43.49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3519, Accuracy: 2267/2500 (90.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0973, Accuracy: 21506/47500 (45.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3309, Accuracy: 2288/2500 (91.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1421, Accuracy: 21311/47500 (44.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3380, Accuracy: 2275/2500 (91.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2325, Accuracy: 21155/47500 (44.54%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.3171, Accuracy: 2301/2500 (92.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1589, Accuracy: 21105/47500 (44.43%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2987, Accuracy: 2325/2500 (93.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1337, Accuracy: 21508/47500 (45.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2739, Accuracy: 2333/2500 (93.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1945, Accuracy: 21439/47500 (45.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2641, Accuracy: 2336/2500 (93.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2870, Accuracy: 21451/47500 (45.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2382, Accuracy: 2364/2500 (94.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3053, Accuracy: 21436/47500 (45.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2652, Accuracy: 2349/2500 (93.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2887, Accuracy: 21047/47500 (44.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2317, Accuracy: 2367/2500 (94.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3602, Accuracy: 21175/47500 (44.58%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2208, Accuracy: 2366/2500 (94.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4750, Accuracy: 21586/47500 (45.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2070, Accuracy: 2386/2500 (95.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4687, Accuracy: 21319/47500 (44.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2012, Accuracy: 2401/2500 (96.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4580, Accuracy: 21283/47500 (44.81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2107, Accuracy: 2382/2500 (95.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5449, Accuracy: 20668/47500 (43.51%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2187, Accuracy: 2393/2500 (95.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4789, Accuracy: 20768/47500 (43.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2137, Accuracy: 2382/2500 (95.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4280, Accuracy: 20884/47500 (43.97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.2019, Accuracy: 2392/2500 (95.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5261, Accuracy: 21125/47500 (44.47%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1725, Accuracy: 2419/2500 (96.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6022, Accuracy: 21365/47500 (44.98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1910, Accuracy: 2412/2500 (96.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.5164, Accuracy: 20865/47500 (43.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1566, Accuracy: 2428/2500 (97.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6480, Accuracy: 21061/47500 (44.34%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1461, Accuracy: 2434/2500 (97.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6091, Accuracy: 21355/47500 (44.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1363, Accuracy: 2437/2500 (97.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.7327, Accuracy: 21419/47500 (45.09%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1658, Accuracy: 2406/2500 (96.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6837, Accuracy: 20956/47500 (44.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1399, Accuracy: 2436/2500 (97.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6995, Accuracy: 21440/47500 (45.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1508, Accuracy: 2424/2500 (96.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.7693, Accuracy: 21279/47500 (44.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1336, Accuracy: 2447/2500 (97.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.6723, Accuracy: 21193/47500 (44.62%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1226, Accuracy: 2445/2500 (97.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8342, Accuracy: 21167/47500 (44.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1159, Accuracy: 2455/2500 (98.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8107, Accuracy: 21189/47500 (44.61%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1099, Accuracy: 2456/2500 (98.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9232, Accuracy: 20950/47500 (44.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1052, Accuracy: 2459/2500 (98.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9452, Accuracy: 20960/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1107, Accuracy: 2463/2500 (98.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9510, Accuracy: 20956/47500 (44.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1024, Accuracy: 2453/2500 (98.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0881, Accuracy: 21221/47500 (44.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1108, Accuracy: 2448/2500 (97.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0137, Accuracy: 21205/47500 (44.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1431, Accuracy: 2425/2500 (97.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9484, Accuracy: 20267/47500 (42.67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1234, Accuracy: 2454/2500 (98.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.8593, Accuracy: 20808/47500 (43.81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1105, Accuracy: 2455/2500 (98.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9692, Accuracy: 21059/47500 (44.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1123, Accuracy: 2456/2500 (98.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9912, Accuracy: 20742/47500 (43.67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.1094, Accuracy: 2461/2500 (98.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.9596, Accuracy: 20858/47500 (43.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0985, Accuracy: 2467/2500 (98.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0011, Accuracy: 21163/47500 (44.55%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0886, Accuracy: 2470/2500 (98.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0926, Accuracy: 21183/47500 (44.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0889, Accuracy: 2470/2500 (98.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0357, Accuracy: 20765/47500 (43.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0854, Accuracy: 2468/2500 (98.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1328, Accuracy: 21027/47500 (44.27%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0780, Accuracy: 2471/2500 (98.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1586, Accuracy: 20969/47500 (44.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0789, Accuracy: 2467/2500 (98.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2943, Accuracy: 20967/47500 (44.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0820, Accuracy: 2468/2500 (98.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2284, Accuracy: 20873/47500 (43.94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0803, Accuracy: 2477/2500 (99.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.0836, Accuracy: 21011/47500 (44.23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0822, Accuracy: 2468/2500 (98.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2046, Accuracy: 20968/47500 (44.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0813, Accuracy: 2464/2500 (98.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2341, Accuracy: 20866/47500 (43.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0731, Accuracy: 2475/2500 (99.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.1373, Accuracy: 20856/47500 (43.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0680, Accuracy: 2481/2500 (99.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.2902, Accuracy: 20627/47500 (43.43%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0660, Accuracy: 2480/2500 (99.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.3090, Accuracy: 20982/47500 (44.17%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0567, Accuracy: 2483/2500 (99.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.4958, Accuracy: 21018/47500 (44.25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0653, Accuracy: 2475/2500 (99.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.4183, Accuracy: 20855/47500 (43.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0614, Accuracy: 2472/2500 (98.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5353, Accuracy: 20837/47500 (43.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0586, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.3628, Accuracy: 20674/47500 (43.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0593, Accuracy: 2480/2500 (99.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.4430, Accuracy: 20728/47500 (43.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0479, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5209, Accuracy: 21034/47500 (44.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0619, Accuracy: 2483/2500 (99.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5698, Accuracy: 20720/47500 (43.62%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0476, Accuracy: 2484/2500 (99.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5732, Accuracy: 21160/47500 (44.55%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0596, Accuracy: 2480/2500 (99.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5995, Accuracy: 20768/47500 (43.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0400, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.5304, Accuracy: 20895/47500 (43.99%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0422, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8087, Accuracy: 20799/47500 (43.79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0449, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6737, Accuracy: 20730/47500 (43.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0413, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7494, Accuracy: 20873/47500 (43.94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0439, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6799, Accuracy: 20744/47500 (43.67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0422, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7384, Accuracy: 20672/47500 (43.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0462, Accuracy: 2483/2500 (99.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7960, Accuracy: 20816/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0404, Accuracy: 2492/2500 (99.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7417, Accuracy: 20733/47500 (43.65%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0394, Accuracy: 2489/2500 (99.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7432, Accuracy: 20887/47500 (43.97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0377, Accuracy: 2489/2500 (99.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8797, Accuracy: 20878/47500 (43.95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0383, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8010, Accuracy: 20621/47500 (43.41%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0359, Accuracy: 2492/2500 (99.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7693, Accuracy: 20910/47500 (44.02%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0405, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6813, Accuracy: 20852/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0381, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7528, Accuracy: 20613/47500 (43.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0369, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7828, Accuracy: 20630/47500 (43.43%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0508, Accuracy: 2485/2500 (99.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8060, Accuracy: 20763/47500 (43.71%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0413, Accuracy: 2485/2500 (99.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8481, Accuracy: 20802/47500 (43.79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0384, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.7759, Accuracy: 20692/47500 (43.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0408, Accuracy: 2486/2500 (99.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9907, Accuracy: 20827/47500 (43.85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0435, Accuracy: 2489/2500 (99.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8162, Accuracy: 20712/47500 (43.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0410, Accuracy: 2491/2500 (99.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8650, Accuracy: 20490/47500 (43.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0362, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0774, Accuracy: 20802/47500 (43.79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0347, Accuracy: 2486/2500 (99.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9318, Accuracy: 20873/47500 (43.94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0324, Accuracy: 2492/2500 (99.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8504, Accuracy: 20844/47500 (43.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0440, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.6548, Accuracy: 20447/47500 (43.05%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0429, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.8880, Accuracy: 20602/47500 (43.37%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0277, Accuracy: 2494/2500 (99.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0103, Accuracy: 20960/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0401, Accuracy: 2487/2500 (99.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9816, Accuracy: 20343/47500 (42.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0374, Accuracy: 2488/2500 (99.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0801, Accuracy: 20577/47500 (43.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0344, Accuracy: 2492/2500 (99.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0521, Accuracy: 20830/47500 (43.85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0325, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0670, Accuracy: 20582/47500 (43.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0275, Accuracy: 2491/2500 (99.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0617, Accuracy: 20899/47500 (44.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0265, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1313, Accuracy: 20889/47500 (43.98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0286, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1434, Accuracy: 20476/47500 (43.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0228, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2349, Accuracy: 20814/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0231, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3003, Accuracy: 20640/47500 (43.45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0282, Accuracy: 2494/2500 (99.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2823, Accuracy: 20848/47500 (43.89%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0231, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3286, Accuracy: 20862/47500 (43.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0251, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1701, Accuracy: 20694/47500 (43.57%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0196, Accuracy: 2492/2500 (99.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3513, Accuracy: 20715/47500 (43.61%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0233, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4108, Accuracy: 20672/47500 (43.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0242, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2450, Accuracy: 20711/47500 (43.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0195, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4806, Accuracy: 20802/47500 (43.79%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0174, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4202, Accuracy: 20660/47500 (43.49%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0229, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2648, Accuracy: 20705/47500 (43.59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0223, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4523, Accuracy: 20707/47500 (43.59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0167, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5057, Accuracy: 20570/47500 (43.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0175, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4353, Accuracy: 20569/47500 (43.30%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0159, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5628, Accuracy: 20776/47500 (43.74%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0229, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3906, Accuracy: 20780/47500 (43.75%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0172, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3708, Accuracy: 20851/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0195, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6145, Accuracy: 20454/47500 (43.06%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0320, Accuracy: 2486/2500 (99.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5071, Accuracy: 20639/47500 (43.45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0257, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1871, Accuracy: 20586/47500 (43.34%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0201, Accuracy: 2493/2500 (99.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5044, Accuracy: 20821/47500 (43.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0266, Accuracy: 2487/2500 (99.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.8016, Accuracy: 20425/47500 (43.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0213, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6149, Accuracy: 20533/47500 (43.23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0182, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5247, Accuracy: 20940/47500 (44.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0142, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6257, Accuracy: 20869/47500 (43.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0131, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6636, Accuracy: 20975/47500 (44.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0164, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.7819, Accuracy: 20712/47500 (43.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0179, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5581, Accuracy: 20785/47500 (43.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0202, Accuracy: 2490/2500 (99.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6815, Accuracy: 20686/47500 (43.55%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0200, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5108, Accuracy: 20517/47500 (43.19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0211, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5874, Accuracy: 20691/47500 (43.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0172, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.8132, Accuracy: 20515/47500 (43.19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0150, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5650, Accuracy: 20586/47500 (43.34%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0139, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6854, Accuracy: 20541/47500 (43.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0176, Accuracy: 2495/2500 (99.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.5904, Accuracy: 20607/47500 (43.38%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0134, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6650, Accuracy: 20641/47500 (43.45%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0150, Accuracy: 2497/2500 (99.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6996, Accuracy: 20496/47500 (43.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0187, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6130, Accuracy: 20242/47500 (42.61%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0154, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6770, Accuracy: 20553/47500 (43.27%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0141, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6332, Accuracy: 20331/47500 (42.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0128, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.0266, Accuracy: 20868/47500 (43.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0136, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.8220, Accuracy: 20791/47500 (43.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0128, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.9903, Accuracy: 20570/47500 (43.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0145, Accuracy: 2498/2500 (99.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.7871, Accuracy: 20615/47500 (43.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0126, Accuracy: 2499/2500 (99.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.8052, Accuracy: 20572/47500 (43.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0139, Accuracy: 2496/2500 (99.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.7866, Accuracy: 20654/47500 (43.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0160, Accuracy: 2493/2500 (99.72%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68e6652ab484bf6b99b670e5f5a5613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='model_B', max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 4.7899, Accuracy: 20571/47500 (43.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5225, Accuracy: 1156/2500 (46.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6210, Accuracy: 20280/47500 (42.69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.4364, Accuracy: 1237/2500 (49.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6031, Accuracy: 21043/47500 (44.30%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3395, Accuracy: 1281/2500 (51.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.5666, Accuracy: 21448/47500 (45.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3508, Accuracy: 1283/2500 (51.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6255, Accuracy: 21289/47500 (44.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.3016, Accuracy: 1332/2500 (53.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6187, Accuracy: 21523/47500 (45.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2829, Accuracy: 1354/2500 (54.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6508, Accuracy: 21492/47500 (45.25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2710, Accuracy: 1368/2500 (54.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6963, Accuracy: 21233/47500 (44.70%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2216, Accuracy: 1386/2500 (55.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6613, Accuracy: 21625/47500 (45.53%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2030, Accuracy: 1417/2500 (56.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.6486, Accuracy: 21726/47500 (45.74%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2294, Accuracy: 1397/2500 (55.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7527, Accuracy: 21391/47500 (45.03%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2282, Accuracy: 1400/2500 (56.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7435, Accuracy: 21150/47500 (44.53%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1780, Accuracy: 1447/2500 (57.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7156, Accuracy: 21791/47500 (45.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1815, Accuracy: 1443/2500 (57.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7606, Accuracy: 21514/47500 (45.29%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1791, Accuracy: 1475/2500 (59.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7803, Accuracy: 21281/47500 (44.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2064, Accuracy: 1448/2500 (57.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8303, Accuracy: 21006/47500 (44.22%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1585, Accuracy: 1480/2500 (59.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7569, Accuracy: 21365/47500 (44.98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1298, Accuracy: 1500/2500 (60.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.7700, Accuracy: 21577/47500 (45.43%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1647, Accuracy: 1459/2500 (58.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8191, Accuracy: 21297/47500 (44.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1858, Accuracy: 1458/2500 (58.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8632, Accuracy: 20957/47500 (44.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1993, Accuracy: 1458/2500 (58.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9109, Accuracy: 20789/47500 (43.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1336, Accuracy: 1507/2500 (60.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8604, Accuracy: 21445/47500 (45.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1440, Accuracy: 1486/2500 (59.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8582, Accuracy: 21358/47500 (44.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1764, Accuracy: 1493/2500 (59.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9108, Accuracy: 21164/47500 (44.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1011, Accuracy: 1537/2500 (61.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8551, Accuracy: 21437/47500 (45.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1324, Accuracy: 1506/2500 (60.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8869, Accuracy: 21250/47500 (44.74%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1594, Accuracy: 1494/2500 (59.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9293, Accuracy: 20982/47500 (44.17%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1147, Accuracy: 1520/2500 (60.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8887, Accuracy: 21314/47500 (44.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1013, Accuracy: 1523/2500 (60.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.8697, Accuracy: 21413/47500 (45.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1160, Accuracy: 1528/2500 (61.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9356, Accuracy: 21187/47500 (44.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1018, Accuracy: 1532/2500 (61.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9034, Accuracy: 21147/47500 (44.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1223, Accuracy: 1527/2500 (61.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9452, Accuracy: 20952/47500 (44.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0938, Accuracy: 1548/2500 (61.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9086, Accuracy: 21427/47500 (45.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1254, Accuracy: 1513/2500 (60.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9494, Accuracy: 21018/47500 (44.25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1172, Accuracy: 1527/2500 (61.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9791, Accuracy: 21246/47500 (44.73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1190, Accuracy: 1503/2500 (60.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9789, Accuracy: 21035/47500 (44.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0784, Accuracy: 1565/2500 (62.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9459, Accuracy: 21350/47500 (44.95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0917, Accuracy: 1550/2500 (62.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9494, Accuracy: 21023/47500 (44.26%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0905, Accuracy: 1554/2500 (62.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9622, Accuracy: 21285/47500 (44.81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1101, Accuracy: 1540/2500 (61.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9776, Accuracy: 20963/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0889, Accuracy: 1545/2500 (61.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9481, Accuracy: 21343/47500 (44.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0745, Accuracy: 1547/2500 (61.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9435, Accuracy: 21306/47500 (44.85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1627, Accuracy: 1493/2500 (59.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0581, Accuracy: 20740/47500 (43.66%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0792, Accuracy: 1534/2500 (61.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9596, Accuracy: 21408/47500 (45.07%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1365, Accuracy: 1509/2500 (60.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0545, Accuracy: 20836/47500 (43.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0774, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9660, Accuracy: 21310/47500 (44.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1269, Accuracy: 1533/2500 (61.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0815, Accuracy: 20971/47500 (44.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0963, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0201, Accuracy: 21051/47500 (44.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0812, Accuracy: 1543/2500 (61.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9951, Accuracy: 21075/47500 (44.37%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1114, Accuracy: 1536/2500 (61.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0478, Accuracy: 21055/47500 (44.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0777, Accuracy: 1524/2500 (60.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.9868, Accuracy: 20869/47500 (43.93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1163, Accuracy: 1545/2500 (61.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0465, Accuracy: 21123/47500 (44.47%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0855, Accuracy: 1563/2500 (62.52%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0275, Accuracy: 21202/47500 (44.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1048, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0313, Accuracy: 21295/47500 (44.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1083, Accuracy: 1547/2500 (61.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0715, Accuracy: 20931/47500 (44.07%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1014, Accuracy: 1555/2500 (62.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0602, Accuracy: 20861/47500 (43.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0875, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0341, Accuracy: 21309/47500 (44.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0921, Accuracy: 1556/2500 (62.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0670, Accuracy: 21183/47500 (44.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0726, Accuracy: 1551/2500 (62.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0201, Accuracy: 21073/47500 (44.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1218, Accuracy: 1528/2500 (61.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1036, Accuracy: 20961/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0923, Accuracy: 1541/2500 (61.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0655, Accuracy: 21165/47500 (44.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1223, Accuracy: 1541/2500 (61.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1033, Accuracy: 20961/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0643, Accuracy: 1554/2500 (62.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0098, Accuracy: 21051/47500 (44.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0895, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0716, Accuracy: 21056/47500 (44.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0968, Accuracy: 1565/2500 (62.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0878, Accuracy: 21056/47500 (44.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1036, Accuracy: 1558/2500 (62.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0683, Accuracy: 21203/47500 (44.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1300, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1139, Accuracy: 20959/47500 (44.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1194, Accuracy: 1546/2500 (61.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0986, Accuracy: 21000/47500 (44.21%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1176, Accuracy: 1541/2500 (61.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1077, Accuracy: 20999/47500 (44.21%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1003, Accuracy: 1545/2500 (61.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0782, Accuracy: 20855/47500 (43.91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1699, Accuracy: 1522/2500 (60.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1752, Accuracy: 20719/47500 (43.62%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1145, Accuracy: 1531/2500 (61.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0979, Accuracy: 20904/47500 (44.01%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1618, Accuracy: 1507/2500 (60.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1833, Accuracy: 20667/47500 (43.51%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0950, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0908, Accuracy: 20999/47500 (44.21%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0821, Accuracy: 1568/2500 (62.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0428, Accuracy: 20846/47500 (43.89%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1187, Accuracy: 1560/2500 (62.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1243, Accuracy: 20911/47500 (44.02%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0904, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.0872, Accuracy: 21247/47500 (44.73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1163, Accuracy: 1551/2500 (62.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1718, Accuracy: 20913/47500 (44.03%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1000, Accuracy: 1594/2500 (63.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1539, Accuracy: 21120/47500 (44.46%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1091, Accuracy: 1544/2500 (61.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1599, Accuracy: 20914/47500 (44.03%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1011, Accuracy: 1569/2500 (62.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1338, Accuracy: 21005/47500 (44.22%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0991, Accuracy: 1553/2500 (62.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1339, Accuracy: 21096/47500 (44.41%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0940, Accuracy: 1571/2500 (62.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1343, Accuracy: 20953/47500 (44.11%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1123, Accuracy: 1555/2500 (62.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1673, Accuracy: 20842/47500 (43.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1186, Accuracy: 1551/2500 (62.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1300, Accuracy: 20889/47500 (43.98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0749, Accuracy: 1589/2500 (63.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1342, Accuracy: 21110/47500 (44.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1033, Accuracy: 1554/2500 (62.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1378, Accuracy: 20935/47500 (44.07%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0849, Accuracy: 1562/2500 (62.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1051, Accuracy: 21083/47500 (44.39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1669, Accuracy: 1491/2500 (59.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1972, Accuracy: 20359/47500 (42.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1241, Accuracy: 1526/2500 (61.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1916, Accuracy: 20967/47500 (44.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0963, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1509, Accuracy: 21126/47500 (44.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0822, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1343, Accuracy: 21047/47500 (44.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0974, Accuracy: 1553/2500 (62.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1269, Accuracy: 20958/47500 (44.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0944, Accuracy: 1546/2500 (61.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1429, Accuracy: 20846/47500 (43.89%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1340, Accuracy: 1539/2500 (61.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1966, Accuracy: 20727/47500 (43.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0950, Accuracy: 1568/2500 (62.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1831, Accuracy: 21020/47500 (44.25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1403, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2056, Accuracy: 20941/47500 (44.09%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0998, Accuracy: 1571/2500 (62.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1796, Accuracy: 20913/47500 (44.03%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1215, Accuracy: 1532/2500 (61.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2147, Accuracy: 20822/47500 (43.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1101, Accuracy: 1559/2500 (62.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2049, Accuracy: 20928/47500 (44.06%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1167, Accuracy: 1555/2500 (62.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1997, Accuracy: 20715/47500 (43.61%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0953, Accuracy: 1580/2500 (63.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1772, Accuracy: 20999/47500 (44.21%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1548, Accuracy: 1539/2500 (61.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2620, Accuracy: 20735/47500 (43.65%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0920, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1654, Accuracy: 20992/47500 (44.19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1499, Accuracy: 1522/2500 (60.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2317, Accuracy: 20733/47500 (43.65%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0938, Accuracy: 1575/2500 (63.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1877, Accuracy: 21056/47500 (44.33%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1241, Accuracy: 1556/2500 (62.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2408, Accuracy: 20814/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1314, Accuracy: 1553/2500 (62.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2187, Accuracy: 20814/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0956, Accuracy: 1589/2500 (63.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1633, Accuracy: 21081/47500 (44.38%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1652, Accuracy: 1529/2500 (61.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3004, Accuracy: 20643/47500 (43.46%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1320, Accuracy: 1540/2500 (61.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1867, Accuracy: 20742/47500 (43.67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1515, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2201, Accuracy: 20854/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1191, Accuracy: 1541/2500 (61.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1952, Accuracy: 20814/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0988, Accuracy: 1550/2500 (62.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1674, Accuracy: 20813/47500 (43.82%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1551, Accuracy: 1533/2500 (61.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2745, Accuracy: 20595/47500 (43.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1269, Accuracy: 1567/2500 (62.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2170, Accuracy: 20823/47500 (43.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1111, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2052, Accuracy: 20917/47500 (44.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1308, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2792, Accuracy: 20900/47500 (44.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1180, Accuracy: 1575/2500 (63.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2262, Accuracy: 20851/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1270, Accuracy: 1567/2500 (62.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2500, Accuracy: 20918/47500 (44.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0963, Accuracy: 1571/2500 (62.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2220, Accuracy: 20930/47500 (44.06%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1305, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2798, Accuracy: 20795/47500 (43.78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1163, Accuracy: 1560/2500 (62.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2862, Accuracy: 20741/47500 (43.67%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1057, Accuracy: 1558/2500 (62.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2456, Accuracy: 20877/47500 (43.95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1335, Accuracy: 1559/2500 (62.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2499, Accuracy: 20837/47500 (43.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1049, Accuracy: 1569/2500 (62.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2279, Accuracy: 20942/47500 (44.09%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1308, Accuracy: 1537/2500 (61.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2180, Accuracy: 20838/47500 (43.87%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1104, Accuracy: 1567/2500 (62.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.1961, Accuracy: 20887/47500 (43.97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1169, Accuracy: 1553/2500 (62.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2577, Accuracy: 20899/47500 (44.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0943, Accuracy: 1573/2500 (62.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2131, Accuracy: 20971/47500 (44.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1231, Accuracy: 1560/2500 (62.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2876, Accuracy: 20889/47500 (43.98%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1135, Accuracy: 1572/2500 (62.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2570, Accuracy: 20973/47500 (44.15%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0919, Accuracy: 1572/2500 (62.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2437, Accuracy: 21045/47500 (44.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1251, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2569, Accuracy: 20771/47500 (43.73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1346, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2748, Accuracy: 20720/47500 (43.62%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1164, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2682, Accuracy: 20919/47500 (44.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1369, Accuracy: 1555/2500 (62.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3214, Accuracy: 20818/47500 (43.83%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1516, Accuracy: 1553/2500 (62.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2748, Accuracy: 20790/47500 (43.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1385, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2631, Accuracy: 20794/47500 (43.78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1322, Accuracy: 1566/2500 (62.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3032, Accuracy: 20789/47500 (43.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1569, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3085, Accuracy: 20692/47500 (43.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1082, Accuracy: 1580/2500 (63.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2237, Accuracy: 21049/47500 (44.31%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1298, Accuracy: 1542/2500 (61.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2152, Accuracy: 20841/47500 (43.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1062, Accuracy: 1577/2500 (63.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2330, Accuracy: 20978/47500 (44.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0831, Accuracy: 1599/2500 (63.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2261, Accuracy: 21063/47500 (44.34%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0962, Accuracy: 1600/2500 (64.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2797, Accuracy: 20929/47500 (44.06%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1266, Accuracy: 1570/2500 (62.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2693, Accuracy: 20860/47500 (43.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1418, Accuracy: 1578/2500 (63.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2995, Accuracy: 20777/47500 (43.74%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1608, Accuracy: 1556/2500 (62.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3111, Accuracy: 20767/47500 (43.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1399, Accuracy: 1567/2500 (62.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3205, Accuracy: 20774/47500 (43.73%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1511, Accuracy: 1576/2500 (63.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3557, Accuracy: 20914/47500 (44.03%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1384, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3382, Accuracy: 20694/47500 (43.57%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1160, Accuracy: 1574/2500 (62.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3101, Accuracy: 20860/47500 (43.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1250, Accuracy: 1582/2500 (63.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2697, Accuracy: 20927/47500 (44.06%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1204, Accuracy: 1572/2500 (62.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3031, Accuracy: 20797/47500 (43.78%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1243, Accuracy: 1584/2500 (63.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3380, Accuracy: 20939/47500 (44.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1354, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2827, Accuracy: 20888/47500 (43.97%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1054, Accuracy: 1586/2500 (63.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2645, Accuracy: 21015/47500 (44.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1158, Accuracy: 1580/2500 (63.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2891, Accuracy: 20974/47500 (44.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1778, Accuracy: 1540/2500 (61.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3804, Accuracy: 20705/47500 (43.59%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1382, Accuracy: 1579/2500 (63.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3176, Accuracy: 20804/47500 (43.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0883, Accuracy: 1599/2500 (63.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2617, Accuracy: 20990/47500 (44.19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1047, Accuracy: 1581/2500 (63.24%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2487, Accuracy: 20822/47500 (43.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1484, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3256, Accuracy: 20783/47500 (43.75%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1618, Accuracy: 1536/2500 (61.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3242, Accuracy: 20536/47500 (43.23%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1187, Accuracy: 1593/2500 (63.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2950, Accuracy: 20880/47500 (43.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1507, Accuracy: 1559/2500 (62.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3244, Accuracy: 20678/47500 (43.53%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1481, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3255, Accuracy: 20761/47500 (43.71%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.0993, Accuracy: 1593/2500 (63.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2355, Accuracy: 20991/47500 (44.19%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1267, Accuracy: 1549/2500 (61.96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3119, Accuracy: 20831/47500 (43.85%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1361, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3048, Accuracy: 20689/47500 (43.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1490, Accuracy: 1559/2500 (62.36%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3547, Accuracy: 20636/47500 (43.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1295, Accuracy: 1564/2500 (62.56%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3144, Accuracy: 20871/47500 (43.94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1470, Accuracy: 1569/2500 (62.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3242, Accuracy: 20699/47500 (43.58%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1769, Accuracy: 1545/2500 (61.80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3893, Accuracy: 20634/47500 (43.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1197, Accuracy: 1558/2500 (62.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2561, Accuracy: 20862/47500 (43.92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.2145, Accuracy: 1533/2500 (61.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3821, Accuracy: 20543/47500 (43.25%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1384, Accuracy: 1567/2500 (62.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2967, Accuracy: 20784/47500 (43.76%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1981, Accuracy: 1540/2500 (61.60%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.4207, Accuracy: 20609/47500 (43.39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1336, Accuracy: 1568/2500 (62.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3100, Accuracy: 20730/47500 (43.64%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1255, Accuracy: 1586/2500 (63.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2934, Accuracy: 20964/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1564, Accuracy: 1543/2500 (61.72%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2868, Accuracy: 20677/47500 (43.53%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1589, Accuracy: 1546/2500 (61.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3204, Accuracy: 20487/47500 (43.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1441, Accuracy: 1552/2500 (62.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3116, Accuracy: 20756/47500 (43.70%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1603, Accuracy: 1542/2500 (61.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2989, Accuracy: 20852/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1501, Accuracy: 1580/2500 (63.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3278, Accuracy: 20790/47500 (43.77%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1347, Accuracy: 1578/2500 (63.12%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2847, Accuracy: 20832/47500 (43.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1278, Accuracy: 1576/2500 (63.04%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2969, Accuracy: 20978/47500 (44.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1702, Accuracy: 1542/2500 (61.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3375, Accuracy: 20611/47500 (43.39%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1344, Accuracy: 1585/2500 (63.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3054, Accuracy: 20832/47500 (43.86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1216, Accuracy: 1585/2500 (63.40%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3036, Accuracy: 20968/47500 (44.14%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1105, Accuracy: 1597/2500 (63.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2938, Accuracy: 20962/47500 (44.13%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1308, Accuracy: 1572/2500 (62.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2982, Accuracy: 20974/47500 (44.16%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1822, Accuracy: 1533/2500 (61.32%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3350, Accuracy: 20779/47500 (43.75%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1543, Accuracy: 1542/2500 (61.68%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.2862, Accuracy: 20848/47500 (43.89%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1502, Accuracy: 1555/2500 (62.20%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3310, Accuracy: 20822/47500 (43.84%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1493, Accuracy: 1561/2500 (62.44%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3337, Accuracy: 20900/47500 (44.00%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1571, Accuracy: 1557/2500 (62.28%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3828, Accuracy: 20752/47500 (43.69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1551, Accuracy: 1577/2500 (63.08%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3105, Accuracy: 20854/47500 (43.90%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1411, Accuracy: 1547/2500 (61.88%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3172, Accuracy: 20810/47500 (43.81%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 1.1417, Accuracy: 1587/2500 (63.48%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 2.3529, Accuracy: 20751/47500 (43.69%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiments_dir = './experiments_cifar_v6'\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "min_samples = 100\n",
    "min_pct = min_samples / len(data_train)\n",
    "max_pct = 0.5\n",
    "print(\"min_pct: {:.3f}\".format(min_pct))\n",
    "\n",
    "# pcts = np.logspace(np.log10(min_pct), np.log10(max_pct), num=10, base=10)\n",
    "pcts = [0.05]\n",
    "print(\"pcts: {}\".format(pcts))\n",
    "\n",
    "for pct in pcts:\n",
    "    print(\"experimenting with pct={}\".format(pct))\n",
    "    config = {\n",
    "        'n_epochs': n_epochs,\n",
    "        'pct': pct\n",
    "    }\n",
    "    with Experiment(config=config, experiments_dir=experiments_dir) as experiment:\n",
    "        config = experiment.config\n",
    "        acc1, acc2, acc3, acc4 = get_reverse_accuracy(pct=config.pct, n_epochs=config.n_epochs)\n",
    "\n",
    "        experiment.register_result(\"acc1s\", acc1)\n",
    "        experiment.register_result(\"acc2s\", acc2)\n",
    "        experiment.register_result(\"acc3s\", acc3)\n",
    "        experiment.register_result(\"acc4s\", acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
